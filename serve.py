import os
import json
import re
import unicodedata
from typing import Optional, Tuple, List, Dict, Any
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv
from difflib import SequenceMatcher

from rag.ingest_faq import ingest_all_faqs
from reflection import Reflection
from semantic_router.semantic import SemanticSearch
from embedding_model.embedding import EmbeddingModel
from config import Config
from utils.logger import setup_logger
from utils.rate_limiter import rate_limit
from utils.validators import MessageValidator
from utils.response import ChatResponse

# Load environment variables
load_dotenv()
default_db = './chroma_db'

# Ingest all FAQ data into ChromaDB before starting the service
print("üîÑ Ingesting all FAQ data into ChromaDB...")
ingest_all_faqs()
print("‚úÖ FAQ ingest completed.")

# Initialize paths and services
db_path = os.getenv('DB_PATH', default_db)

# Initialize Flask app
app = Flask(
    __name__,
    static_url_path='/static',
    static_folder='static',
    template_folder='templates'
)
CORS(app, resources={
    r"/*": {
        "origins": ["http://127.0.0.1:5000", "http://localhost:5000"],
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type", "Accept"]
    }
})

# Setup logging
logger = setup_logger()

# Initialize embedding and semantic search models
embedding_model = EmbeddingModel()
semantic_search = SemanticSearch()

class FAQMatcher:
    def __init__(self):
        # Blacklist phrases that should be filtered out immediately
        self.no_answer_phrases = [
            'th·ªùi ti·∫øt','ch√≠nh tr·ªã','th·ªÉ thao','kinh t·∫ø vƒ© m√¥','ƒë·∫£ng ph√°i','livestream','3D','government policy','ch√≠nh s√°ch nh√† n∆∞·ªõc','tourism','du l·ªãch','hotel','kh√°ch s·∫°n',
            'vi·ªát nam c√≥ bao nhi√™u t·ªânh','to√°n h·ªçc','v·∫≠t l√Ω','c·ªù b·∫°c','ƒëua xe','h√†ng gi·∫£','LGBT','how many provinces does vietnam have','vi·ªát nam c√≥ bao nhi√™u t·ªânh','weather','th·ªùi ti·∫øt','climate change','bi·∫øn ƒë·ªïi kh√≠ h·∫≠u',
            'h√≥a h·ªçc','sinh h·ªçc','l·ªãch s·ª≠','ƒë·ªãa l√Ω','vƒÉn h·ªçc','kim ti√™m','h·ªçc ph√≠','gi√° cao','population of vietnam','d√¢n s·ªë vi·ªát nam','flight','chuy·∫øn bay','c·∫£ng h√†ng kh√¥ng',
            't√™n b·∫°n l√† g√¨','b·∫°n bao nhi√™u tu·ªïi', 'b·∫°n s·ªëng ·ªü ƒë√¢u','l√†m ti·ªÅn','b√™ ƒë√™','qu·ªëc ph√≤ng','stock market', 'ch·ªâ s·ªë ch·ª©ng kho√°n','bus schedule', 'l·ªãch xe bu√Ωt', 'giao th√¥ng',
            'b·∫°n c√≥ ng∆∞·ªùi y√™u ch∆∞a','b·∫°n ƒë√£ k·∫øt h√¥n ch∆∞a','c√°ch m·∫°ng','covid','qu√¢n s·ª±','bi·ªÉn','ƒë·∫£o','inflation rate', 't·ª∑ l·ªá l·∫°m ph√°t','music festival', 'l·ªÖ h·ªôi √¢m nh·∫°c','sports','th·ªÉ thao','football', 'b√≥ng ƒë√°', 'basketball',
            'b·∫°n ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o', 'thu·∫≠t to√°n c·ªßa b·∫°n l√† g√¨','ƒë·∫£o ch√≠nh','c·∫ßm t√π','ho√†ng xa','tr∆∞·ªùng xa','exchange rate', 't·ª∑ gi√° ngo·∫°i t·ªá','c√¥ng th·ª©c n·∫•u ƒÉn'
            'b·∫°n h·ªçc nh∆∞ th·∫ø n√†o','b·∫°n l√† model g√¨', 'd·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa b·∫°n l√† g√¨','ƒë√°nh','khai chi·∫øn','concert', 'bu·ªïi h√≤a nh·∫°c', 'movie', 'phim','health', 's·ª©c kh·ªèe', 'y t·∫ø',
            'nh√† h√†ng','cƒÉng tin','k√Ω t√∫c x√°', 'nh√† ·ªü','c·ªìn','ch·∫•t k√≠ch th√≠ch','m·∫°i d√¢m','xung ƒë·ªôt','qu·ªëc gia', 'd√¢n t·ªôc','how do you work', 'bot ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o',
            'giao th√¥ng','l·ªãch xe bu√Ωt','b√£i ƒë·∫≠u xe','l·ªãch thi','bia','politics', 'ch√≠nh tr·ªã', 'ƒë·∫£ng ph√°i', 'kinh t·∫ø vƒ© m√¥','exercise', 'th·ªÉ d·ª•c', 'gym',
       
        ]

        # Library domain keywords
        self.LIBRARY_KEYWORDS = {
            'library', 'book', 'borrow', 'return', 'renew',
            'resources', 'wifi', 'computer', 'printer',
            'find', 'lost item', 'library card',
            'contact', 'rules', 'services', 'guidelines',
            'th∆∞ vi·ªán', 's√°ch', 'm∆∞·ª£n', 'tr·∫£', 'ƒë·ªçc', 't√†i li·ªáu',
            'sinh vi√™n', 'gi·∫£ng vi√™n', 'tr∆∞·ªùng', 'ƒë·∫°i h·ªçc',
            'tdtu', 'th·∫ª', 'ph√≠', 'ph·∫°t', 'gia h·∫°n', 'ƒëƒÉng k√Ω',
            't√†i kho·∫£n', 'c∆° s·ªü', 'ph√≤ng', 'gi·ªù', 'm·ªü c·ª≠a',
            'ƒë√≥ng c·ª≠a', 'd·ªãch v·ª•', 'nghi√™n c·ª©u', 'h·ªçc t·∫≠p',
            'thi', 'k·ª≥ thi', 'khoa', 'ng√†nh'
        }

        # Abbreviation and keyword maps
        self.tech_terms = {
            'm√°y t√≠nh': ['computer', 'laptop', 'pc'],
            'm√°y in': ['printer'],
            'm√°y qu√©t': ['scanner'],
            'photo': ['copy'],
            'wifi': ['internet'],
            't·∫£i': ['download', 'upload'],
            'ƒëƒÉng nh·∫≠p': ['login'],
            't√†i kho·∫£n': ['account'],
            'm·∫≠t kh·∫©u': ['password']
        }
        self.abbrev_map = {
            'tdt': 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng',
            'tv': 'th∆∞ vi·ªán',
            'sv': 'sinh vi√™n',
        }
        self.keyword_map = {
            'printer': 'dichvu',
            'scanner': 'huongdan',
            'wifi': 'dichvu',
            'download': 'huongdan',
            'upload': 'huongdan',
            'login': 'huongdan',
            'lost': 'lienhe',
            'm·∫•t': 'lienhe',
            'th·∫•t l·∫°c': 'lienhe',
            'qu√™n': 'lienhe',
            'forgot': 'lienhe',
            'missing': 'lienhe'
        }

        # Regex patterns to "predict" categories from user queries
        self.question_patterns = {
            'instructions': [r'(how|how do i|how can i)', r'cach .*', r'where .*', r'where .*'],
            'quydinh': [r'quy ƒë·ªãnh .*', r'(co c√≥|d·ª©c ƒë∆∞·ª£c|d·ª©c ƒë∆∞·ª£c|rules).*'],
            'dichvu': [r'ƒëi·ªÅu sƒ© .*', r'(ph√¨|gi√°|services).*'],
            'muon_tra': [r'(m∆∞·ª£n|tu·∫ßn| xu√¢n|borrow|return|renew).*'],
            'lianhe': [r'(li·ªÖn li√™n|contact|g·∫£i ai).*'],
            'chitchat': [r'(hello|hi|hello|goodbye|bye).*']
        }
        
        # Strict chitchat patterns - only basic greetings and thanks
        self.chitchat_patterns = {
            r'\b(xin ch√†o|hello|hi|hey|ch√†o)\b': 'greeting',
            r'\b(c·∫£m ∆°n|thank|thanks|c√°m ∆°n)\b': 'thanks',
            r'\b(t·∫°m bi·ªát|bye|goodbye)\b': 'goodbye'
        }

        # Load FAQ data and build hash map for direct lookup
        self.faq_data = self.load_faq_data()
        self.faq_map = self.build_faq_map()

        # Build semantic index on FAQ questions
        questions = [entry['question'] for entry in self.faq_data]
        semantic_search.build_index(questions)

        # Define allowed specialized categories for semantic search
        self.allowed_specialized = {'huongdan', 'quydinh', 'muon_tra', 'dichvu', 'lienhe'}

    def in_library_domain(self, text: str) -> bool:
        """Check if the text is related to library domain."""
        text = text.lower()
        return any(kw in text for kw in self.LIBRARY_KEYWORDS)

    def load_faq_data(self) -> List[Dict[str, Any]]:
        try:
            with open("faq_data.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                return data
        except Exception as e:
            logger.error(f"Error loading FAQ data: {e}")
            return []

    def normalize_text(self, text: str) -> str:
        """Normalize text by removing accents, punctuation, and extra whitespace."""
        # 1) NFKD for accent separation
        text = unicodedata.normalize('NFKD', text)
        text = ''.join(ch for ch in text if not unicodedata.combining(ch))
        # 2) Lower, strip punctuation
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # remove .,;?‚Ä¶
        text = re.sub(r'\s+', ' ', text).strip()  # collect whitespace
        return text

    def build_faq_map(self) -> Dict[str, Dict[str, Any]]:
        faq_map = {}
        for entry in self.faq_data:
            # Normalize main question
            raw = entry['question']
            norm = self.normalize_text(raw)
            faq_map[norm] = entry
            
            # Normalize aliases too
            for alias in entry.get('aliases', []):
                norm_alias = self.normalize_text(alias)
                faq_map[norm_alias] = entry
        return faq_map

    def normalize_abbreviations(self, text: str) -> str:
        q = text.lower().strip()
        # Remove trailing punctuation
        q = re.sub(r'[?!.]+$', '', q)
        # Expand abbreviations
        for abbr, full in self.abbrev_map.items():
            q = re.sub(rf"\b{re.escape(abbr)}\b", full, q)
        return q

    def find_best_match(self, user_question: str, threshold: float = 0.9) -> Dict[str, Any]:
        norm = self.normalize_text(user_question)

        # 1) Direct lookup / alias
        if norm in self.faq_map:
            return {**self.faq_map[norm], 'confidence': 1.0}

        # 2) Fuzzy-match alias
        best_ratio, best_entry = 0.0, None
        for key, ent in self.faq_map.items():
            r = SequenceMatcher(None, norm, key).ratio()
            if r > best_ratio:
                best_ratio, best_entry = r, ent
        if best_ratio >= 0.85:
            return {**best_entry, 'confidence': best_ratio}

        # 3) Keyword map
        for kw, cat in self.keyword_map.items():
            if kw in norm:
                return {'question': user_question, 'category': cat, 'confidence': 1.0}

        # 4) Domain filter: if not in library domain, ignore semantics
        if not self.in_library_domain(norm):
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

        # 5) Semantic fallback only for allowed categories
        try:
            results = semantic_search.query(user_question, top_k=1, threshold=threshold)
            if results:
                idx, score = results[0]['index'], float(results[0]['score'])
                matched = self.faq_data[idx].copy()
                if matched.get('category') in self.allowed_specialized and score >= threshold:
                    matched['confidence'] = score
                    return matched
        except Exception as e:
            logger.error(f"Error in semantic matching: {e}")

        # 6) No match
        return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

    def get_best_response(self, user_message: str) -> Tuple[str, str, float]:
        match = self.find_best_match(user_message)
        answer = match.get('answer')
        category = match.get('category')
        confidence = match.get('confidence', 0.0)

        # 1) Direct/alias or keyword match with given answer
        if confidence == 1.0 and answer:
            return answer, category, confidence

        # 2) Only chatbot dialogue when truly chit-chat
        if category == 'chitchat':
            conv = Reflection(db_path=db_path).chat(None, user_message)
            return conv, 'chatbot', 0.0

        # 3) Fuzzy or semantic success
        if confidence > 0.0:
            return answer or "", category, confidence

        # 4) Default: not understood
        return (
            "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c ƒë·∫∑t c√¢u h·ªèi kh√°c nh√©.",
            'unknown',
            0.0
        )

# Instantiate matcher and chatbot
faq_matcher = FAQMatcher()
chatbot = Reflection(db_path=Config.DB_PATH)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
@rate_limit
def chat():
    try:
        data = request.get_json() or {}
        msg = data.get("message", "").strip()
        session_id = data.get("session_id", "default_session")
        if not msg:
            return jsonify({"error": Config.MESSAGES['empty_message']}), 400

        answer, category, confidence = faq_matcher.get_best_response(msg)
        return jsonify(ChatResponse(
            message=answer,
            category=category,
            confidence=confidence
        ).to_dict())
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}", exc_info=True)
        return jsonify({"error": Config.MESSAGES['server_error']}), 500

@app.route('/favicon.ico')
def favicon():
    return send_from_directory(
        os.path.dirname(os.path.abspath(__file__)),
        'favicon.ico',
        mimetype='image/vnd.microsoft.icon'
    )

@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Not found"}), 404

@app.errorhandler(500)
def server_error(e):
    logger.error(f"Server error: {e}", exc_info=True)
    return jsonify({"error": Config.MESSAGES['server_error']}), 500

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=Config.DEBUG_MODE)
