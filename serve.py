import os
import json
import re
import unicodedata
from typing import Optional, Tuple, List, Dict, Any
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv
from difflib import SequenceMatcher
from rapidfuzz import fuzz, process
import numpy as np

from rag.ingest_faq import ingest_all_faqs
from reflection import Reflection
from semantic_router.semantic import SemanticSearch
from embedding_model.embedding import EmbeddingModel
from config import Config
from utils.logger import setup_logger
from utils.rate_limiter import rate_limit
from utils.validators import MessageValidator
from utils.response import ChatResponse

# Load environment variables
load_dotenv()
default_db = './chroma_db'

# Ingest all FAQ data into ChromaDB before starting the service
print("üîÑ Ingesting all FAQ data into ChromaDB...")
ingest_all_faqs()
print("‚úÖ FAQ ingest completed.")

# Initialize paths and services
db_path = os.getenv('DB_PATH', default_db)

# Initialize Flask app
app = Flask(
    __name__,
    static_url_path='/static',
    static_folder='static',
    template_folder='templates'
)
CORS(app)  # Allow all origins during development

# Setup logging
logger = setup_logger()

# Initialize embedding and semantic search models
embedding_model = EmbeddingModel()
semantic_search = SemanticSearch()

class FAQMatcher:
    def __init__(self):
        # Blacklist phrases that should be filtered out immediately
        self.no_answer_phrases = [
            'th·ªùi ti·∫øt','ch√≠nh tr·ªã','th·ªÉ thao','kinh t·∫ø vƒ© m√¥','ƒë·∫£ng ph√°i','livestream','3D','government policy','ch√≠nh s√°ch nh√† n∆∞·ªõc','tourism','du l·ªãch','hotel','kh√°ch s·∫°n','c√° ƒë·ªô','ƒë·∫∑t c∆∞·ª£c'
            'vi·ªát nam c√≥ bao nhi√™u t·ªânh','to√°n h·ªçc','v·∫≠t l√Ω','c·ªù b·∫°c','ƒëua xe','h√†ng gi·∫£','LGBT','how many provinces does vietnam have','vi·ªát nam c√≥ bao nhi√™u t·ªânh','weather','th·ªùi ti·∫øt','climate change','bi·∫øn ƒë·ªïi kh√≠ h·∫≠u',
            'h√≥a h·ªçc','sinh h·ªçc','l·ªãch s·ª≠','ƒë·ªãa l√Ω','vƒÉn h·ªçc','kim ti√™m','h·ªçc ph√≠','gi√° cao','population of vietnam','d√¢n s·ªë vi·ªát nam','flight','chuy·∫øn bay','c·∫£ng h√†ng kh√¥ng','c√° c∆∞·ª£c','t·ª≠ s√°t'
            't√™n b·∫°n l√† g√¨','b·∫°n bao nhi√™u tu·ªïi', 'b·∫°n s·ªëng ·ªü ƒë√¢u','l√†m ti·ªÅn','b√™ ƒë√™','qu·ªëc ph√≤ng','stock market', 'ch·ªâ s·ªë ch·ª©ng kho√°n','bus schedule', 'l·ªãch xe bu√Ωt', 'giao th√¥ng','t·ª± t·ª≠'
            'b·∫°n c√≥ ng∆∞·ªùi y√™u ch∆∞a','b·∫°n ƒë√£ k·∫øt h√¥n ch∆∞a','c√°ch m·∫°ng','covid','qu√¢n s·ª±','bi·ªÉn','ƒë·∫£o','inflation rate', 't·ª∑ l·ªá l·∫°m ph√°t','music festival', 'l·ªÖ h·ªôi √¢m nh·∫°c','sports','th·ªÉ thao','football', 'b√≥ng ƒë√°', 'basketball',
            'b·∫°n ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o', 'thu·∫≠t to√°n c·ªßa b·∫°n l√† g√¨','ƒë·∫£o ch√≠nh','c·∫ßm t√π','ho√†ng xa','tr∆∞·ªùng xa','exchange rate', 't·ª∑ gi√° ngo·∫°i t·ªá','c√¥ng th·ª©c n·∫•u ƒÉn','b√™ ƒë√™','thi√™u s·ªëng'
            'b·∫°n h·ªçc nh∆∞ th·∫ø n√†o','b·∫°n l√† model g√¨', 'd·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa b·∫°n l√† g√¨','ƒë√°nh','khai chi·∫øn','concert', 'bu·ªïi h√≤a nh·∫°c', 'movie', 'phim','health', 's·ª©c kh·ªèe', 'y t·∫ø',
            'nh√† h√†ng','cƒÉng tin','k√Ω t√∫c x√°', 'nh√† ·ªü','c·ªìn','ch·∫•t k√≠ch th√≠ch','m·∫°i d√¢m','xung ƒë·ªôt','qu·ªëc gia', 'd√¢n t·ªôc','how do you work', 'bot ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o','gi·∫øt','s√°ch c·∫•m'
            'giao th√¥ng','l·ªãch xe bu√Ωt','b√£i ƒë·∫≠u xe','l·ªãch thi','bia','politics', 'ch√≠nh tr·ªã', 'ƒë·∫£ng ph√°i', 'kinh t·∫ø vƒ© m√¥','exercise', 'th·ªÉ d·ª•c', 'gym','nh·∫°y c·∫£m','ch√©m','ƒë√¢m','c√°ch m·∫°ng'
        ]

        # Common typos mapping
        self.common_typos = {
            'th·ªùi ti·∫øt': ['thoi tiet', 'th·ªùi ti·∫øt', 'th·ªùi ti√™t'],
            'ch√≠nh tr·ªã': ['chinh tri', 'ch√≠nh tr·ªã', 'ch√≠nh tr·ªã'],
            'th·ªÉ thao': ['the thao', 'th·ªÉ thao', 'th·ªÉ thao'],
            'du l·ªãch': ['du lich', 'du l·ªãch', 'du l·ªãch'],
            'kh√°ch s·∫°n': ['khach san', 'kh√°ch s·∫°n', 'kh√°ch s·∫°n'],
            'th·ªÉ d·ª•c': ['the duc', 'th·ªÉ d·ª•c', 'th·ªÉ d·ª•c'],
            'giao th√¥ng': ['giao thong', 'giao th√¥ng', 'giao th√¥ng'],
            'ch√≠nh s√°ch': ['chinh sach', 'ch√≠nh s√°ch', 'ch√≠nh s√°ch'],
            'qu·ªëc gia': ['quoc gia', 'qu·ªëc gia', 'qu·ªëc gia'],
            'd√¢n t·ªôc': ['dan toc', 'd√¢n t·ªôc', 'd√¢n t·ªôc'],
            'Th∆∞ vi·ªán': ['thu vien', 'th∆∞ vi·ªán','th∆∞ vi·ªán inspire',"INSPiRE",'th∆∞ vi·ªán INSPiRE','Th∆∞ vi·ªán Truy·ªÅn c·∫£m h·ª©ng'],
            'Tr∆∞·ªùng ƒê·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': ['tdtu', 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng', 'TDTU','tr∆∞·ªùng tdt','Tr∆∞·ªùng Ton Duc Thang','ƒë·∫°i h·ªçc Ton Duc Thang'],
            'ƒê·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': ['tdtu', 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng', 'TDTU'],
        }   

        # Ignored patterns for super normalization
        self.IGNORED_PATTERNS = [
            r'\b·ªü th∆∞ l√Ω\b', r'\b√†i th∆∞ l√Ω\b', r'\btrong th∆∞ l√Ω\b',
            r'\btoi muon\b', r'\btoi can\b', r'\bgiu\b', r'\bcho em h·ªèi\b', r'\bxin h·ªèi\b', r'\bl√†m xin\b', r'\bxin\b',
            r'\bbai l√†\b', r'\bai ƒëang\b', r'\bai r·ªìi\b', r'\bai ƒëi\b', r'\bai ƒëi ch·ªß\b',
            r'\bnay v·ªÅ\b', r'\bchi ‚Äã‚Äã‚Äã‚Äãti·∫øt\b', r'\b th√¥ng tin v·ªÅ\b',
            r'\bvui l√≤ng\b', r'\bl√†m ∆°n\b', r'\bgi√∫p\b', r'\bgi√∫p t√¥i\b',
            r'\bcho t√¥i bi·∫øt\b', r'\bcho t√¥i h·ªèi\b', r'\bcho h·ªèi\b',
            r'\bc√≥ th·ªÉ\b', r'\bc√≥ ƒë∆∞·ª£c kh√¥ng\b', r'\bc√≥ ph·∫£i\b',
            r'\bƒë∆∞·ª£c kh√¥ng\b', r'\bph·∫£i kh√¥ng\b', r'\bƒë√∫ng kh√¥ng\b'
        ]

        # Synonym mapping for better matching
        self.SYNONYM_MAP = {
            'm∆∞·ª£n': ['vay', 'l·∫•y', 'nh·∫≠n', 'ƒë·ªçc'],
            'tr·∫£': ['ho√†n tr·∫£', 'n·ªôp l·∫°i', 'g·ª≠i l·∫°i'],
            't√†i li·ªáu': ['s√°ch', 's√°ch b√°o', 'gi√°o tr√¨nh', 't√†i li·ªáu h·ªçc t·∫≠p'],
            'gi·∫£ng vi√™n': ['th·∫ßy c√¥', 'gi√°o vi√™n', 'gi·∫£ng vi√™n'],
            'sinh vi√™n': ['h·ªçc vi√™n', 'sinh vi√™n'],
            'th∆∞ vi·ªán': ['th∆∞ vi·ªán tr∆∞·ªùng', 'th∆∞ vi·ªán tdtu'],
            'm√°y t√≠nh': ['m√°y vi t√≠nh', 'computer', 'pc'],
            'm√°y in': ['printer', 'm√°y photocopy'],
            'wifi': ['internet', 'm·∫°ng', 'k·∫øt n·ªëi m·∫°ng'],
            'ƒëƒÉng nh·∫≠p': ['login', 'ƒëƒÉng k√Ω', 'truy c·∫≠p'],
            't√†i kho·∫£n': ['account', 'user'],
            'm·∫≠t kh·∫©u': ['password', 'pass'],
            'ph√≠': ['gi√°', 'chi ph√≠', 'ti·ªÅn'],
            'gia h·∫°n': ['renew', 'k√©o d√†i', 'th√™m th·ªùi gian'],
            'quy ƒë·ªãnh': ['n·ªôi quy', 'lu·∫≠t', 'quy t·∫Øc'],
            'd·ªãch v·ª•': ['service', 'ti·ªán √≠ch'],
            'h∆∞·ªõng d·∫´n': ['guide', 'manual', 'tutorial'],
            'li√™n h·ªá': ['contact', 'g·∫∑p', 'g·∫∑p m·∫∑t'],
            'm·∫•t': ['th·∫•t l·∫°c', 'qu√™n', 'kh√¥ng t√¨m th·∫•y'],
            't√¨m': ['search', 't√¨m ki·∫øm', 'locate'],
            'h·ªçc qua ƒë√™m': ['h·ªçc ban ƒë√™m', 'h·ªçc khuya', 'h·ªçc t·ªëi', 'h·ªçc ƒë√™m', 'h·ªçc xuy√™n ƒë√™m'],
            'khu v·ª±c h·ªçc qua ƒë√™m': ['khu v·ª±c h·ªçc ban ƒë√™m', 'khu v·ª±c h·ªçc khuya', 'ph√≤ng h·ªçc qua ƒë√™m', 'ph√≤ng h·ªçc ban ƒë√™m', 'ph√≤ng h·ªçc khuya'],
            'ƒëƒÉng k√Ω h·ªçc qua ƒë√™m': ['ƒëƒÉng k√Ω h·ªçc ban ƒë√™m', 'ƒëƒÉng k√Ω h·ªçc khuya', 'ƒë·∫∑t ph√≤ng h·ªçc qua ƒë√™m', 'ƒë·∫∑t ph√≤ng h·ªçc ban ƒë√™m', 'ƒë·∫∑t ph√≤ng h·ªçc khuya']
        }

        # Library domain keywords
        self.LIBRARY_KEYWORDS = {
            # Basic library terms
            'th∆∞ vi·ªán', 'library', 'thu vien',
            's√°ch', 'book', 'books', 'sach',
            't√†i li·ªáu', 'tai lieu', 'document', 'documents',
            'gi√°o tr√¨nh', 'giao trinh', 'textbook', 'textbooks',
            't·∫°p ch√≠', 'tap chi', 'journal', 'journals',
            'b√°o', 'bao', 'newspaper', 'newspapers',
            
            # Library services
            'm∆∞·ª£n', 'muon', 'borrow', 'borrowing',
            'tr·∫£', 'tra', 'return', 'returning',
            'ƒë·ªçc', 'doc', 'read', 'reading',
            'wifi', 'internet', 'm·∫°ng', 'mang',
            'm√°y t√≠nh', 'may tinh', 'computer', 'computers',
            'm√°y in', 'may in', 'printer', 'printers',
            'm√°y qu√©t', 'may quet', 'scanner', 'scanners',
            'photo', 'photocopy', 'copy', 'copying',
            
            # Library operations
            'gi·ªù m·ªü c·ª≠a', 'gio mo cua', 'opening hours', 'opening time',
            'gi·ªù ƒë√≥ng c·ª≠a', 'gio dong cua', 'closing hours', 'closing time',
            'ƒëƒÉng k√Ω', 'dang ky', 'register', 'registration',
            'ƒëƒÉng nh·∫≠p', 'dang nhap', 'login', 'sign in',
            't√†i kho·∫£n', 'tai khoan', 'account', 'accounts',
            'm·∫≠t kh·∫©u', 'mat khau', 'password', 'passwords',
            'th·∫ª', 'the', 'card', 'cards',
            'ph√≠', 'phi', 'fee', 'fees',
            'gia h·∫°n', 'gia han', 'renew', 'renewal',
            
            # Library spaces
            'ph√≤ng', 'phong', 'room', 'rooms',
            'khu v·ª±c', 'khu vuc', 'area', 'areas',
            't·∫ßng', 'tang', 'floor', 'floors',
            't√≤a nh√†', 'toa nha', 'building', 'buildings',
            
            # Library users
            'sinh vi√™n', 'sinh vien', 'student', 'students',
            'gi·∫£ng vi√™n', 'giang vien', 'lecturer', 'lecturers',
            'c√°n b·ªô', 'can bo', 'staff', 'staffs',
            
            # Library rules
            'quy ƒë·ªãnh', 'quy dinh', 'rule', 'rules',
            'n·ªôi quy', 'noi quy', 'regulation', 'regulations',
            'h∆∞·ªõng d·∫´n', 'huong dan', 'guide', 'guidelines',
            
            # Library resources
            'c∆° s·ªü d·ªØ li·ªáu', 'co so du lieu', 'database', 'databases',
            't√†i nguy√™n', 'tai nguyen', 'resource', 'resources',
            't√†i li·ªáu s·ªë', 'tai lieu so', 'digital resource', 'digital resources',
            
            # Library services
            'd·ªãch v·ª•', 'dich vu', 'service', 'services',
            'h·ªó tr·ª£', 'ho tro', 'support', 'assistance',
            't∆∞ v·∫•n', 'tu van', 'consultation', 'advice',
            
            # Library locations
            'c∆° s·ªü', 'co so', 'campus', 'campuses',
            'chi nh√°nh', 'chi nhanh', 'branch', 'branches',
            
            # Library operations
            'm·ªü c·ª≠a', 'mo cua', 'open', 'opening',
            'ƒë√≥ng c·ª≠a', 'dong cua', 'close', 'closing',
            'ngh·ªâ', 'nghi', 'closed', 'holiday',
            'l√†m vi·ªác', 'lam viec', 'working', 'operating',
            
            # Library specific
            'INSPiRE', 'inspire', 'th∆∞ vi·ªán truy·ªÅn c·∫£m h·ª©ng',
            'TVƒêHTƒêT', 'th∆∞ vi·ªán ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng',
            'TDTU', 'tdtu', 'ton duc thang'
        }

        # Expanded tech terms mapping
        self.tech_terms = {
            'm√°y t√≠nh': ['computer', 'laptop', 'pc'],
            'm√°y in': ['printer'],
            'm√°y qu√©t': ['scanner'],
            'photo': ['copy'],
            'wifi': ['internet'],
            't·∫£i': ['download', 'upload'],
            'ƒëƒÉng nh·∫≠p': ['login'],
            't√†i kho·∫£n': ['account'],
            'm·∫≠t kh·∫©u': ['password'],
            # Reference management software
            'endnote': 'huongdan',
            'zotero': 'huongdan',
            'mendeley': 'huongdan',
            'reference manager': 'huongdan',
            'citation': 'huongdan',
            'tr√≠ch d·∫´n': 'huongdan',
            't√†i li·ªáu tham kh·∫£o': 'huongdan',
            # Database access
            'database': 'huongdan',
            'c∆° s·ªü d·ªØ li·ªáu': 'huongdan',
            'proquest': 'huongdan',
            'ebsco': 'huongdan',
            'ieee': 'huongdan',
            'science direct': 'huongdan',
            'springer': 'huongdan',
            # Research tools
            'turnitin': 'huongdan',
            'plagiarism': 'huongdan',
            'ƒë·∫°o vƒÉn': 'huongdan',
            'research': 'huongdan',
            'nghi√™n c·ª©u': 'huongdan'
        }
        self.abbrev_map = {
            'tdt': 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng',
            'tv': 'th∆∞ vi·ªán',
            'sv': 'sinh vi√™n',
            'gv': 'Gi·∫£ng vi√™n',
            'GV-VC': 'Gi·∫£ng vi√™n-Vi√™n ch·ª©c',
            'c·ª±u sv': 'C·ª±u Sinh vi√™n',
            'hvch': 'H·ªçc vi√™n cao h·ªçc',
            'ncs' : 'Ngi√™n c·ª©u sinh',
            'ht' : 'Hi·ªáu tr∆∞·ªüng',
            'csdl': 'C∆° s·ªü d·ªØ li·ªáu',
            'CSDL' : 'C∆° s·ªü d·ªØ li·ªáu',
            'th·∫ª sv': 'Th·∫ª sinh vi√™n',
            'th·∫ª tv': 'Th·∫ª th∆∞ vi·ªán',
            'database': 'C∆° s·ªü d·ªØ li·ªáu',
            'copyright': 'B·∫£n quy·ªÅn',
            'Th∆∞ vi·ªán ƒë·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': 'TVƒêHTƒêT'
        }
        self.keyword_map = {
            'printer': 'dichvu',
            'scanner': 'huongdan',
            'wifi': 'dichvu',
            'download': 'huongdan',
            'upload': 'huongdan',
            'login': 'huongdan',
            'lost': 'lienhe',
            'm·∫•t': 'lienhe',
            'th·∫•t l·∫°c': 'lienhe',
            'qu√™n': 'lienhe',
            'forgot': 'lienhe',
            'missing': 'lienhe'
        }

        # Regex patterns to "predict" categories from user queries
        self.question_patterns = {
            'instructions': [r'(how|how do i|how can i)', r'cach .*', r'where .*', r'where .*'],
            'quydinh': [r'quy ƒë·ªãnh .*', r'(co c√≥|d·ª©c ƒë∆∞·ª£c|d·ª©c ƒë∆∞·ª£c|rules).*'],
            'dichvu': [r'ƒëi·ªÅu sƒ© .*', r'(ph√¨|gi√°|services).*'],
            'muon_tra': [r'(m∆∞·ª£n|tu·∫ßn| xu√¢n|borrow|return|renew).*'],
            'lianhe': [r'(li·ªÖn li√™n|contact|g·∫£i ai).*'],
            'chitchat': [r'(hello|hi|hello|goodbye|bye).*'],
            'plagiarism': [
                r'(ki·ªÉm tra|check).*(tr√πng l·∫∑p|ƒë·∫°o vƒÉn|sao ch√©p)',
                r'(c√≥|h·ªó tr·ª£).*(ki·ªÉm tra|check).*(tr√πng l·∫∑p|ƒë·∫°o vƒÉn)',
                r'(c√°ch|l√†m sao).*(ki·ªÉm tra|check).*(tr√πng l·∫∑p|ƒë·∫°o vƒÉn)'
            ],
            'publication': [
                r'(c√¥ng b·ªë|b√†i b√°o).*(qu·ªëc t·∫ø|international)',
                r'(ki·ªÉm tra|check).*(c√¥ng b·ªë|b√†i b√°o)',
                r'(h·ªó tr·ª£|support).*(c√¥ng b·ªë|publication)'
            ]
        }
        
        # Strict chitchat patterns - only basic greetings and thanks
        self.chitchat_patterns = {
            r'\b(xin ch√†o|hello|hi|hey|ch√†o)\b': 'greeting',
            r'\b(c·∫£m ∆°n|thank|thanks|c√°m ∆°n)\b': 'thanks',
            r'\b(t·∫°m bi·ªát|bye|goodbye)\b': 'goodbye'
        }

        # Load FAQ data and build hash map for direct lookup
        self.faq_data = self.load_faq_data()
        
        # Build normalized questions and aliases for fuzzy matching
        self.norm_questions = [
            (self.super_normalize(self.expand_synonyms(q['question'])), i)
            for i, q in enumerate(self.faq_data)
        ]
        self.norm_aliases = [
            (self.super_normalize(self.expand_synonyms(alias)), i)
            for i, q in enumerate(self.faq_data)
            for alias in q.get('aliases', [])
        ]

        # Build semantic index on FAQ questions
        questions = [entry['question'] for entry in self.faq_data]
        semantic_search.build_index(questions)

        # Define allowed specialized categories for semantic search
        self.allowed_specialized = {'huongdan', 'quydinh', 'muon_tra', 'dichvu', 'lienhe'}

    def remove_accents(self, text: str) -> str:
        """Remove Vietnamese accents from text."""
        nfkd_form = unicodedata.normalize('NFKD', text)
        return "".join([c for c in nfkd_form if not unicodedata.combining(c)])

    def is_blocked_phrase(self, user_message: str, threshold: int = 80) -> bool:
        """Check if user message contains any blocked phrases using fuzzy matching."""
        if not user_message:
            return False
            
        user_message_lower = user_message.lower().strip()
        user_message_no_accents = self.remove_accents(user_message_lower)
        
        # List of university-related terms that should never be blocked
        university_terms = {
            'tdtu', 'ton duc thang', 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng',
            'fibaa', 'hc√©res', 'aacsb', 'abest', 'acbsp',
            'ch·ª©ng nh·∫≠n', 'chung nhan', 'certification',
            'ki·ªÉm ƒë·ªãnh', 'kiem dinh', 'accreditation',
            'x·∫øp h·∫°ng', 'xep hang', 'ranking',
            'ƒë√°nh gi√°', 'danh gia', 'assessment',
            'ch·∫•t l∆∞·ª£ng', 'chat luong', 'quality',
            'ƒë√†o t·∫°o', 'dao tao', 'education',
            'gi·∫£ng d·∫°y', 'giang day', 'teaching',
            'nghi√™n c·ª©u', 'nghien cuu', 'research',
            'khoa h·ªçc', 'khoa hoc', 'science',
            'qu·ªëc t·∫ø', 'quoc te', 'international'
        }

        # List of library-related terms that should never be blocked
        library_terms = {
            's√°ch', 'sach', 'book', 'books',
            'ph√≤ng', 'phong', 'room', 'rooms',
            'th∆∞ vi·ªán', 'thu vien', 'library',
            'm∆∞·ª£n', 'muon', 'borrow',
            'tr·∫£', 'tra', 'return',
            'ƒë·ªçc', 'doc', 'read',
            't√†i li·ªáu', 'tai lieu', 'document',
            'gi√°o tr√¨nh', 'giao trinh', 'textbook',
            't·∫°p ch√≠', 'tap chi', 'journal',
            'b√°o', 'bao', 'newspaper',
            'm√°y t√≠nh', 'may tinh', 'computer',
            'm√°y in', 'may in', 'printer',
            'wifi', 'internet', 'm·∫°ng',
            'ƒëƒÉng k√Ω', 'dang ky', 'register',
            'ƒëƒÉng nh·∫≠p', 'dang nhap', 'login',
            't√†i kho·∫£n', 'tai khoan', 'account',
            'm·∫≠t kh·∫©u', 'mat khau', 'password',
            'th·∫ª', 'the', 'card',
            'ph√≠', 'phi', 'fee',
            'gia h·∫°n', 'gia han', 'renew',
            'quy ƒë·ªãnh', 'quy dinh', 'rule',
            'd·ªãch v·ª•', 'dich vu', 'service',
            'h∆∞·ªõng d·∫´n', 'huong dan', 'guide',
            'li√™n h·ªá', 'lien he', 'contact',
            'c√¥ng ngh·ªá', 'cong nghe', 'technology',
            'h·ªá th·ªëng', 'he thong', 'system',
            'ph·∫ßn m·ªÅm', 'phan mem', 'software',
            '·ª©ng d·ª•ng', 'ung dung', 'application',
            'c√≤n', 'con', 'still', 'continue',
            'm·ªü c·ª≠a', 'mo cua', 'open',
            'ƒë√≥ng c·ª≠a', 'dong cua', 'close',
            'gi·ªù', 'gio', 'time', 'hour',
            'th·ªùi gian', 'thoi gian', 'time'
        }

        # Combine all protected terms
        protected_terms = library_terms.union(university_terms)
        
        # List of sensitive terms that should always be blocked
        sensitive_terms = {
            'nh·∫°y c·∫£m', 'nhay cam', 'sensitive',
            'ch√≠nh tr·ªã', 'chinh tri', 'politics',
            'ƒë·∫£ng ph√°i', 'dang phai', 'political party',
            'kinh t·∫ø vƒ© m√¥', 'kinh te vi mo', 'macroeconomic',
            'th·ªÉ d·ª•c', 'the duc', 'exercise',
            'gym', 'fitness', 
            'giao th√¥ng', 'giao thong', 'traffic',
            'l·ªãch xe bu√Ωt', 'lich xe buyt', 'bus schedule',
            'b√£i ƒë·∫≠u xe', 'bai dau xe', 'parking',
            'l·ªãch thi', 'lich thi', 'exam schedule',
            'bia', 'beer', 'c·∫ßm ƒë·ªì',
            'nh√† h√†ng', 'nha hang', 'restaurant',
            'cƒÉng tin', 'cang tin', 'canteen',
            'k√Ω t√∫c x√°', 'ky tuc xa', 'dormitory',
            'nh√† ·ªü', 'nha o', 'housing',
            'c·ªìn', 'con', 'alcohol',
            'ch·∫•t k√≠ch th√≠ch', 'chat kich thich', 'stimulant',
            'm·∫°i d√¢m', 'mai dam', 'prostitution',
            'xung ƒë·ªôt', 'xung dot', 'conflict',
            'qu·ªëc gia', 'quoc gia', 'nation',
            'd√¢n t·ªôc', 'dan toc', 'ethnicity'
        }
        
        # First check for sensitive terms - these should always be blocked
        for term in sensitive_terms:
            # Use word boundary check to prevent partial matches
            # Add Vietnamese word boundary check
            if re.search(r'(?<!\w)' + re.escape(term) + r'(?!\w)', user_message_lower) or \
               re.search(r'(?<!\w)' + re.escape(term) + r'(?!\w)', user_message_no_accents):
                # Double check that it's not a protected term
                if not any(prot_term in user_message_lower for prot_term in protected_terms):
                    logger.info(f"Blocked sensitive term: '{term}'")
                    return True
                
        # Then check exact matches for other blocked phrases
        for phrase in self.no_answer_phrases:
            # Skip if the phrase is part of a protected term
            if any(term in user_message_lower for term in protected_terms):
                continue
                
            # Use word boundary check to prevent partial matches
            # Add Vietnamese word boundary check
            if re.search(r'(?<!\w)' + re.escape(phrase) + r'(?!\w)', user_message_lower):
                logger.info(f"Blocked phrase match: '{phrase}'")
                return True
                
        # Then check fuzzy matches with higher threshold for longer phrases
        for phrase in self.no_answer_phrases:
            # Skip short phrases to avoid false positives
            if len(phrase) < 4:
                continue
                
            # Skip if the phrase is part of a protected term
            if any(term in phrase for term in protected_terms):
                continue
                
            # Calculate threshold based on phrase length and content
            phrase_threshold = min(threshold, 85) if len(phrase) > 10 else threshold
            
            # Increase threshold for questions containing special characters or acronyms
            if re.search(r'[A-Z]', user_message) or re.search(r'[^a-z0-9\s]', user_message):
                phrase_threshold = min(phrase_threshold + 5, 95)
            
            # Check original phrase
            ratio = fuzz.partial_ratio(phrase, user_message_lower)
            if ratio >= phrase_threshold:
                # Double check that it's not a protected term
                if not any(term in user_message_lower for term in protected_terms):
                    logger.info(f"Blocked phrase fuzzy match: '{phrase}' with ratio {ratio}")
                    return True
                    
            # Check phrase without accents
            phrase_no_accents = self.remove_accents(phrase)
            ratio = fuzz.partial_ratio(phrase_no_accents, user_message_no_accents)
            if ratio >= phrase_threshold:
                # Double check that it's not a protected term
                if not any(term in user_message_no_accents for term in protected_terms):
                    logger.info(f"Blocked phrase fuzzy match (no accents): '{phrase}' with ratio {ratio}")
                    return True
                    
        return False

    def super_normalize(self, text: str) -> str:
        """Super normalize text by removing auxiliary phrases and normalizing."""
        t = text.lower().strip()
        t = re.sub(r'[^\w\s]', ' ', t)
        for p in self.IGNORED_PATTERNS:
            t = re.sub(p, ' ', t)
        t = re.sub(r'\s+', ' ', t)
        return t.strip()

    def expand_synonyms(self, text: str) -> str:
        """Expand synonyms in text to their canonical form."""
        t = text.lower()
        # First expand using existing synonym map
        for canonical, synonyms in self.SYNONYM_MAP.items():
            for syn in synonyms:
                t = t.replace(syn, canonical)
        
        # Then expand using new synonym mappings
        new_synonyms = {
            "m∆∞·ª£n": ["borrow", "take", "checkout"],
            "tr·∫£": ["return", "give back"],
            "s√°ch": ["book", "t√†i li·ªáu"]
        }
        
        for word, syns in new_synonyms.items():
            for syn in syns:
                if syn in t:
                    t = t.replace(syn, word)
                    
        return t

    def fuzzy_lookup(self, norm_user: str, norm_questions: List[Tuple[str, int]], threshold: int = 85) -> Optional[int]:
        """Find best fuzzy match using rapidfuzz with semantic relevance check."""
        candidates = [q[0] for q in norm_questions]
        
        # First try exact match
        for norm, idx in norm_questions:
            if norm == norm_user:
                return idx
                
        # Then try token set ratio for partial matches
        result = process.extractOne(
            norm_user, 
            candidates, 
            scorer=fuzz.token_set_ratio,
            score_cutoff=threshold
        )
        
        if result:
            matched_text, score, _ = result
            
            # Additional semantic relevance check
            if score >= threshold:
                # Extract key terms from both texts
                user_terms = set(norm_user.split())
                matched_terms = set(matched_text.split())
                
                # Check for key term overlap
                common_terms = user_terms.intersection(matched_terms)
                
                # Define topic-specific terms with more specific keywords
                topic_keywords = {
                    'visit': {
                        'tham quan', 'thƒÉm', 'visit', 'tour', 'kh√°ch', 'ng∆∞·ªùi ngo√†i', 'outsider',
                        'kh√°ch ngo√†i tr∆∞·ªùng', 'kh√°ch thƒÉm', 'tour guide', 'h∆∞·ªõng d·∫´n vi√™n',
                        'ƒëƒÉng k√Ω tham quan', 'ƒë·∫∑t l·ªãch tham quan', 'l·ªãch tham quan'
                    },
                    'wifi': {
                        'wifi', 'internet', 'm·∫°ng', 'm·∫≠t kh·∫©u', 'password', 'k·∫øt n·ªëi', 'connect',
                        'truy c·∫≠p wifi', 'm·∫≠t kh·∫©u wifi', 'k·∫øt n·ªëi m·∫°ng', 'internet access'
                    },
                    'borrow': {
                        'm∆∞·ª£n', 'tr·∫£', 's√°ch', 't√†i li·ªáu', 'borrow', 'return', 'book',
                        'ƒëem v·ªÅ', 'mang v·ªÅ', 'take home', 'bring home'
                    },
                    'access': {
                        'th·∫ª', 'card', 'truy c·∫≠p', 'access', 'v√†o', 's·ª≠ d·ª•ng', 'use',
                        'th·∫ª sinh vi√™n', 'th·∫ª th∆∞ vi·ªán', 'library card', 'student card'
                    },
                    'facility': {
                        'ph√≤ng', 'room', 'khu v·ª±c', 'area', 'c∆° s·ªü v·∫≠t ch·∫•t', 'facility',
                        'ph√≤ng h·ªçc', 'ph√≤ng nghi√™n c·ª©u', 'ph√≤ng thuy·∫øt tr√¨nh'
                    },
                    'service': {
                        'd·ªãch v·ª•', 'service', 'h·ªó tr·ª£', 'support', 't∆∞ v·∫•n', 'consultation',
                        'scan', 'photocopy', 'copy', 'in ·∫•n', 'printing'
                    },
                    'leadership': {
                        'hi·ªáu tr∆∞·ªüng', 'hi·ªáu ph√≥', 'tr∆∞·ªüng', 'ph√≥', 'ch·ªß t·ªãch', 'gi√°m ƒë·ªëc',
                        'dean', 'principal', 'president', 'director'
                    },
                    'night_study': {
                        'h·ªçc qua ƒë√™m', 'h·ªçc ban ƒë√™m', 'h·ªçc khuya', 'h·ªçc t·ªëi', 'h·ªçc ƒë√™m',
                        'khu v·ª±c h·ªçc qua ƒë√™m', 'khu v·ª±c h·ªçc ban ƒë√™m', 'khu v·ª±c h·ªçc khuya',
                        'ph√≤ng h·ªçc qua ƒë√™m', 'ph√≤ng h·ªçc ban ƒë√™m', 'ph√≤ng h·ªçc khuya',
                        'ƒëƒÉng k√Ω h·ªçc qua ƒë√™m', 'ƒëƒÉng k√Ω h·ªçc ban ƒë√™m', 'ƒëƒÉng k√Ω h·ªçc khuya',
                        'th·ªùi gian h·ªçc qua ƒë√™m', 'th·ªùi gian h·ªçc ban ƒë√™m', 'th·ªùi gian h·ªçc khuya'
                    },
                    'plagiarism': {
                        'ki·ªÉm tra tr√πng l·∫∑p', 'ki·ªÉm tra ƒë·∫°o vƒÉn', 'check ƒë·∫°o vƒÉn', 
                        'turnitin', 'ithenticate', 'plagiarism', 'tr√πng l·∫∑p',
                        'sao ch√©p', 'copy', 'duplicate', 'check tr√πng l·∫∑p',
                        'ki·ªÉm tra sao ch√©p', 'ki·ªÉm tra b√†i b√°o', 'check b√†i b√°o'
                    },
                    'publication': {
                        'c√¥ng b·ªë', 'b√†i b√°o', 't·∫°p ch√≠', 'journal', 'publication',
                        'nghi√™n c·ª©u', 'research', 'paper', 'article', 'b√†i vi·∫øt',
                        'c√¥ng b·ªë qu·ªëc t·∫ø', 'international publication'
                    }
                }
                
                # Check which topics are present in user question
                user_topics = set()
                for topic, terms in topic_keywords.items():
                    if any(term in user_terms for term in terms):
                        user_topics.add(topic)
                
                # Check which topics are present in matched question
                matched_topics = set()
                for topic, terms in topic_keywords.items():
                    if any(term in matched_terms for term in terms):
                        matched_topics.add(topic)
                
                # Require at least one common topic
                if not user_topics.intersection(matched_topics):
                    logger.info(f"Rejected fuzzy match due to topic mismatch. User topics: {user_topics}, Matched topics: {matched_topics}")
                    return None
                
                # Special handling for specific topics
                if 'visit' in user_topics:
                    if 'visit' not in matched_topics:
                        logger.info("Rejected fuzzy match for visit question - no visit-related terms")
                        return None
                    # For visit questions, require higher threshold and more specific terms
                    if score < 90:
                        logger.info("Rejected fuzzy match for visit question - score too low")
                        return None
                    # Additional check for visit-specific terms
                    visit_specific_terms = {'kh√°ch ngo√†i tr∆∞·ªùng', 'ng∆∞·ªùi ngo√†i', 'outsider', 'tour guide'}
                    if any(term in user_terms for term in visit_specific_terms):
                        if not any(term in matched_terms for term in visit_specific_terms):
                            logger.info("Rejected fuzzy match for outsider visit question - no specific terms")
                            return None
                
                if 'wifi' in user_topics:
                    if 'wifi' not in matched_topics:
                        logger.info("Rejected fuzzy match for wifi question - no wifi-related terms")
                        return None
                    # For wifi questions, require more specific terms
                    wifi_specific_terms = {'m·∫≠t kh·∫©u', 'password', 'truy c·∫≠p', 'access'}
                    if any(term in user_terms for term in wifi_specific_terms):
                        if not any(term in matched_terms for term in wifi_specific_terms):
                            logger.info("Rejected fuzzy match for wifi access question - no specific terms")
                            return None
                
                if 'borrow' in user_topics:
                    if 'borrow' not in matched_topics:
                        logger.info("Rejected fuzzy match for borrowing question - no borrow-related terms")
                        return None
                
                if 'leadership' in user_topics:
                    if 'leadership' not in matched_topics:
                        logger.info("Rejected fuzzy match for leadership question - no leadership-related terms")
                        return None
                    # For leadership questions, require higher threshold
                    if score < 90:
                        logger.info("Rejected fuzzy match for leadership question - score too low")
                        return None
                
                # Require more common terms for better matching
                if len(common_terms) >= 2:  # Require at least 2 common terms
                    # Find the index of the matched question
                    for norm, idx in norm_questions:
                        if norm == matched_text:
                            return idx
                            
        return None

    def in_library_domain(self, text: str) -> bool:
        """Check if the text is related to library domain using comprehensive keyword matching."""
        # Normalize the text
        text = self.normalize_text(text)
        
        # Check for exact keyword matches
        for kw in self.LIBRARY_KEYWORDS:
            if kw in text:
                return True
                
        # Check for compound terms (e.g., "th∆∞ vi·ªán tr∆∞·ªùng", "th∆∞ vi·ªán tdtu")
        compound_terms = [
            'th∆∞ vi·ªán tr∆∞·ªùng', 'th∆∞ vi·ªán tdtu', 'th∆∞ vi·ªán ƒë·∫°i h·ªçc',
            'library tdtu', 'tdtu library', 'university library'
        ]
        for term in compound_terms:
            if term in text:
                return True
                
        # Check for common library-related phrases
        library_phrases = [
            'm∆∞·ª£n s√°ch', 'tr·∫£ s√°ch', 'ƒë·ªçc s√°ch',
            'th·∫ª th∆∞ vi·ªán', 'th·∫ª sinh vi√™n',
            'gi·ªù m·ªü c·ª≠a', 'gi·ªù ƒë√≥ng c·ª≠a',
            'ƒëƒÉng k√Ω th·∫ª', 'gia h·∫°n s√°ch'
        ]
        for phrase in library_phrases:
            if phrase in text:
                return True
                
        return False

    def load_faq_data(self) -> List[Dict[str, Any]]:
        try:
            with open("faq_data.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                return data
        except Exception as e:
            logger.error(f"Error loading FAQ data: {e}")
            return []

    def normalize_text(self, text: str) -> str:
        """Normalize text by removing accents, punctuation, and extra whitespace."""
        # 1) Strip and lowercase
        text = text.strip().lower()
        
        # 2) Normalize unicode composition
        text = unicodedata.normalize('NFC', text)
        
        # 3) Remove Vietnamese accents
        text = ''.join(c for c in unicodedata.normalize('NFD', text) 
                      if unicodedata.category(c) != 'Mn')
        
        # 4) Remove punctuation and special characters
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # 5) Expand abbreviations using existing map
        for abbr, full in self.abbrev_map.items():
            text = re.sub(rf'\b{re.escape(abbr)}\b', full, text)
            
        # 6) Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def build_faq_map(self) -> Dict[str, Dict[str, Any]]:
        """Create lookup table for questions and aliases with normalized keys."""
        faq_map = {}
        for entry in self.faq_data:
            # Get all questions including aliases
            all_questions = [entry['question']] + entry.get('aliases', [])
            
            # Add each question and its normalized form to the map
            for q in all_questions:
                # Store original question
                faq_map[q.lower()] = entry
                
                # Store normalized version
                norm_q = self.normalize_text(q)
                faq_map[norm_q] = entry
                
                # Generate and store paraphrases
                paraphrases = self.generate_paraphrases(q)
                for p in paraphrases:
                    faq_map[p.lower()] = entry
                    faq_map[self.normalize_text(p)] = entry
                    
        return faq_map

    def generate_paraphrases(self, question: str) -> List[str]:
        """Generate paraphrases for a question to improve matching."""
        paraphrases = []
        
        # Basic template-based paraphrases
        templates = [
            "T√¥i mu·ªën bi·∫øt {q}",
            "Cho t√¥i bi·∫øt {q}",
            "B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt {q}",
            "Xin h·ªèi {q}",
            "T√¥i c·∫ßn bi·∫øt {q}",
            "L√†m sao ƒë·ªÉ {q}",
            "C√°ch {q}",
            "H∆∞·ªõng d·∫´n {q}",
            "Gi·∫£i th√≠ch {q}",
            "Th√¥ng tin v·ªÅ {q}"
        ]
        
        # Add template-based paraphrases
        for template in templates:
            paraphrases.append(template.format(q=question.lower()))
            
        # Add common Vietnamese variations
        variations = {
            "l√†m th·∫ø n√†o": ["c√°ch n√†o", "l√†m sao", "l√†m g√¨", "ph·∫£i l√†m g√¨"],
            "t√¥i mu·ªën": ["t√¥i c·∫ßn", "t√¥i mu·ªën bi·∫øt", "t√¥i c·∫ßn bi·∫øt"],
            "cho t√¥i": ["cho em", "cho m√¨nh", "cho t√¥i bi·∫øt"], 
            "xin h·ªèi": ["cho h·ªèi", "cho t√¥i h·ªèi", "t√¥i mu·ªën h·ªèi"]
        }
        
        # Add variation-based paraphrases
        for original, replacements in variations.items():
            if original in question.lower():
                for replacement in replacements:
                    paraphrases.append(question.lower().replace(original, replacement))
                    
        return list(set(paraphrases))  # Remove duplicates

    def normalize_abbreviations(self, text: str) -> str:
        q = text.lower().strip()
        # Remove trailing punctuation
        q = re.sub(r'[?!.]+$', '', q)
        # Expand abbreviations
        for abbr, full in self.abbrev_map.items():
            q = re.sub(rf"\b{re.escape(abbr)}\b", full, q)
        return q

    def semantic_search(self, user_question: str, threshold: float = 0.75) -> Optional[Tuple[int, float]]:
        """Find closest question based on semantic similarity (cosine)."""
        try:
            # Add specific handling for plagiarism and publication questions
            if any(term in user_question.lower() for term in ['tr√πng l·∫∑p', 'ƒë·∫°o vƒÉn', 'sao ch√©p', 'plagiarism']):
                # Increase threshold for plagiarism-related questions
                threshold = 0.85
                
            if any(term in user_question.lower() for term in ['c√¥ng b·ªë', 'b√†i b√°o', 'publication']):
                # Increase threshold for publication-related questions
                threshold = 0.85
            
            # Get user question embedding
            user_emb = embedding_model.get_embedding(self.normalize_text(user_question))
            
            # Get all question embeddings
            question_embeddings = []
            for entry in self.faq_data:
                question_emb = embedding_model.get_embedding(self.normalize_text(entry['question']))
                question_embeddings.append(question_emb)
                
            # Calculate similarities
            scores = []
            for emb in question_embeddings:
                # Calculate cosine similarity
                dot_product = np.dot(user_emb, emb)
                norm_user = np.linalg.norm(user_emb)
                norm_emb = np.linalg.norm(emb)
                similarity = dot_product / (norm_user * norm_emb)
                scores.append(similarity)
                
            # Find best match
            best_idx = int(np.argmax(scores))
            
            # Additional relevance check
            if scores[best_idx] >= threshold:
                # Check for specific question types
                user_text = self.normalize_text(user_question)
                matched_text = self.normalize_text(self.faq_data[best_idx]['question'])
                
                # Define topic keywords with more specific terms
                topic_keywords = {
                    'hours': {'gi·ªù', 'th·ªùi gian', 'm·ªü c·ª≠a', 'ƒë√≥ng c·ª≠a', 'ho·∫°t ƒë·ªông', 'cu·ªëi tu·∫ßn', 'weekend'},
                    'wifi': {'wifi', 'internet', 'm·∫°ng', 'm·∫≠t kh·∫©u', 'password'},
                    'borrow': {
                        'm∆∞·ª£n', 'tr·∫£', 's√°ch', 't√†i li·ªáu', 'borrow', 'return', 'book',
                        'ƒëem v·ªÅ', 'mang v·ªÅ', 'take home', 'bring home',
                        'gia h·∫°n', 'renew', 'extension',
                        'th·ªùi h·∫°n', 'deadline', 'due date',
                        's·ªë l∆∞·ª£ng', 'quantity', 'limit',
                        'ph√≠', 'fee', 'charge'
                    },
                    'access': {'th·∫ª', 'card', 'truy c·∫≠p', 'access', 'v√†o', 's·ª≠ d·ª•ng', 'use'},
                    'student': {'sinh vi√™n', 'student', 't√¢n sinh vi√™n', 'new student', 'h·ªçc vi√™n'},
                    'facility': {'ph√≤ng', 'room', 'khu v·ª±c', 'area', 'c∆° s·ªü v·∫≠t ch·∫•t', 'facility'},
                    'service': {
                        'd·ªãch v·ª•', 'service', 'h·ªó tr·ª£', 'support', 't∆∞ v·∫•n', 'consultation',
                        'scan', 'photocopy', 'copy', 'in ·∫•n', 'printing',
                        'm√°y in', 'printer', 'm√°y qu√©t', 'scanner'
                    },
                    'leadership': {
                        'hi·ªáu tr∆∞·ªüng', 'hi·ªáu ph√≥', 'tr∆∞·ªüng', 'ph√≥', 'ch·ªß t·ªãch', 'gi√°m ƒë·ªëc',
                        'dean', 'principal', 'president', 'director'
                    },
                    'night_study': {
                        'h·ªçc qua ƒë√™m', 'h·ªçc ban ƒë√™m', 'h·ªçc khuya', 'h·ªçc t·ªëi', 'h·ªçc ƒë√™m',
                        'khu v·ª±c h·ªçc qua ƒë√™m', 'khu v·ª±c h·ªçc ban ƒë√™m', 'khu v·ª±c h·ªçc khuya',
                        'ph√≤ng h·ªçc qua ƒë√™m', 'ph√≤ng h·ªçc ban ƒë√™m', 'ph√≤ng h·ªçc khuya',
                        'ƒëƒÉng k√Ω h·ªçc qua ƒë√™m', 'ƒëƒÉng k√Ω h·ªçc ban ƒë√™m', 'ƒëƒÉng k√Ω h·ªçc khuya',
                        'th·ªùi gian h·ªçc qua ƒë√™m', 'th·ªùi gian h·ªçc ban ƒë√™m', 'th·ªùi gian h·ªçc khuya'
                    },
                    'plagiarism': {
                        'ki·ªÉm tra tr√πng l·∫∑p', 'ki·ªÉm tra ƒë·∫°o vƒÉn', 'check ƒë·∫°o vƒÉn', 
                        'turnitin', 'ithenticate', 'plagiarism', 'tr√πng l·∫∑p',
                        'sao ch√©p', 'copy', 'duplicate', 'check tr√πng l·∫∑p',
                        'ki·ªÉm tra sao ch√©p', 'ki·ªÉm tra b√†i b√°o', 'check b√†i b√°o'
                    },
                    'publication': {
                        'c√¥ng b·ªë', 'b√†i b√°o', 't·∫°p ch√≠', 'journal', 'publication',
                        'nghi√™n c·ª©u', 'research', 'paper', 'article', 'b√†i vi·∫øt',
                        'c√¥ng b·ªë qu·ªëc t·∫ø', 'international publication'
                    }
                }
                
                # Check which topics are present in user question
                user_topics = set()
                for topic, keywords in topic_keywords.items():
                    if any(kw in user_text for kw in keywords):
                        user_topics.add(topic)
                
                # Check which topics are present in matched answer
                matched_topics = set()
                for topic, keywords in topic_keywords.items():
                    if any(kw in matched_text for kw in keywords):
                        matched_topics.add(topic)
                
                # Require at least one common topic
                if not user_topics.intersection(matched_topics):
                    logger.info(f"Rejected match due to topic mismatch. User topics: {user_topics}, Matched topics: {matched_topics}")
                    return None, None
                
                # Special handling for borrowing questions
                if 'borrow' in user_topics:
                    if 'borrow' not in matched_topics:
                        logger.info("Rejected match for borrowing question - no borrow-related topics")
                        return None, None
                    # Additional check for take-home related terms
                    take_home_terms = {'ƒëem v·ªÅ', 'mang v·ªÅ', 'take home', 'bring home'}
                    if any(term in user_text for term in take_home_terms):
                        if not any(term in matched_text for term in take_home_terms):
                            logger.info("Rejected match for take-home question - no take-home related terms")
                            return None, None
                
                # Special handling for student access questions
                if 'student' in user_topics and 'access' in user_topics:
                    if not ('student' in matched_topics or 'access' in matched_topics):
                        logger.info("Rejected match for student access question - no relevant topics")
                        return None, None
                
                logger.info(f"Found semantic match with score {scores[best_idx]}: {self.faq_data[best_idx]['question']}")
                logger.info(f"Topics - User: {user_topics}, Matched: {matched_topics}")
                return best_idx, scores[best_idx]
                
            return None, None
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}", exc_info=True)
            return None, None

    def _find_publication_plagiarism_match(self, user_question: str) -> Dict[str, Any]:
        """Find best match for publication plagiarism checking questions."""
        # Normalize the question
        user_norm = self.super_normalize(self.expand_synonyms(user_question))
        
        # Define specific keywords for publication plagiarism
        plagiarism_keywords = {
            'tr√πng l·∫∑p', 'ƒë·∫°o vƒÉn', 'sao ch√©p', 'plagiarism',
            'ki·ªÉm tra', 'check', 'verify', 'x√°c minh'
        }
        publication_keywords = {
            'c√¥ng b·ªë', 'b√†i b√°o', 'publication', 'paper',
            'qu·ªëc t·∫ø', 'international', 'journal', 'article'
        }
        
        # Check if question contains both plagiarism and publication terms
        has_plagiarism = any(kw in user_norm for kw in plagiarism_keywords)
        has_publication = any(kw in user_norm for kw in publication_keywords)
        
        if not (has_plagiarism and has_publication):
            # If missing either term, return no match
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}
            
        # Search for exact matches first
        for norm, idx in self.norm_questions + self.norm_aliases:
            if norm == user_norm:
                entry = self.faq_data[idx].copy()
                entry['confidence'] = 1.0
                return entry
                
        # Try fuzzy matching with higher threshold
        idx = self.fuzzy_lookup(user_norm, self.norm_questions + self.norm_aliases, threshold=90)
        if idx is not None:
            entry = self.faq_data[idx].copy()
            entry['confidence'] = 0.95
            return entry
            
        # Try semantic search with higher threshold
        try:
            idx, score = self.semantic_search(user_question, threshold=0.85)
            if idx is not None:
                matched = self.faq_data[idx].copy()
                matched['confidence'] = score
                return matched
        except Exception as e:
            logger.error(f"Error in semantic search for publication plagiarism: {e}")
            
        # No match found
        return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

    def find_best_match(self, user_question: str, threshold: float = 0.5) -> Dict[str, Any]:
        """Find the best matching FAQ entry for a user question."""
        # Add context awareness for plagiarism and publication questions
        if any(term in user_question.lower() for term in ['tr√πng l·∫∑p', 'ƒë·∫°o vƒÉn', 'sao ch√©p', 'plagiarism']):
            # Check if it's related to publications
            if any(term in user_question.lower() for term in ['c√¥ng b·ªë', 'b√†i b√°o', 'publication']):
                # Look for specific plagiarism checking for publications
                return self._find_publication_plagiarism_match(user_question)
            
        # 1. Check for blocked phrases using fuzzy matching
        if self.is_blocked_phrase(user_question):
            logger.info(f"Question blocked: {user_question}")
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0, '_no_answer': True}

        # 2. Super normalize and expand synonyms
        user_norm = self.super_normalize(self.expand_synonyms(user_question))
        logger.info(f"Normalized question: {user_norm}")

        # 3. Direct matches (exact match after normalization)
        for norm, idx in self.norm_questions + self.norm_aliases:
            if user_norm == norm:
                entry = self.faq_data[idx].copy()
                entry['confidence'] = 1.0
                logger.info(f"Found exact match: {entry['question']}")
                return entry

        # 4. Fuzzy matches with semantic relevance check
        idx = self.fuzzy_lookup(user_norm, self.norm_questions + self.norm_aliases)
        if idx is not None:
            entry = self.faq_data[idx].copy()
            entry['confidence'] = 0.95
            logger.info(f"Found fuzzy match: {entry['question']}")
            return entry

        # 5. Domain filter: if not in library domain, ignore semantics
        if not self.in_library_domain(user_norm):
            logger.info(f"Question not in library domain: {user_norm}")
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

        # 6. Semantic match using cosine similarity with topic relevance check
        try:
            idx, score = self.semantic_search(user_question, threshold=threshold)
            if idx is not None:
                matched = self.faq_data[idx].copy()
                logger.info(f"Top semantic match: idx={idx}, score={score}, category={matched.get('category')}")
                
                # Only accept if category is allowed and score is high enough
                if matched.get('category') in self.allowed_specialized and score >= threshold:
                    matched['confidence'] = score
                    return matched
                else:
                    logger.info(f"Semantic match rejected: category={matched.get('category')}, score={score}")
        except Exception as e:
            logger.error(f"Error in semantic matching: {e}", exc_info=True)

        # 7. No match found
        logger.info(f"No match found for: {user_question}")
        return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

    def get_best_response(self, user_message: str) -> Tuple[str, str, float]:
        match = self.find_best_match(user_message)
        
        # N·∫øu ·ªü b∆∞·ªõc yes/no m√† kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi
        if match.get('_no_answer'):
            return "Xin l·ªói, t√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi n√†y.", 'unknown', 0.0

        answer = match.get('answer')
        category = match.get('category')
        conf = match.get('confidence', 0.0)

        # 1) FAQ exact/alias ho·∫∑c fuzzy match v·ªõi answer
        if (conf >= 0.95 or conf == 1.0) and answer:
            return answer, category, conf

        # 2) FAQ category keyword nh∆∞ng kh√¥ng c√≥ answer
        if conf >= 0.95 and not answer:
            fallback_md = (
                "Xin l·ªói, t√¥i ch∆∞a ho√†n to√†n hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n, "
                "B·∫°n c√≥ th·ªÉ tr√¨nh b√†y c√¢u h·ªèi m·ªôt c√°ch r√µ r√†ng h∆°n kh√¥ng "
                "ho·∫∑c ƒë·ªÉ l·∫°i c√¢u h·ªèi tr√™n live chat "
                "[Fanpage th∆∞ vi·ªán](https://www.facebook.com/tvdhtdt)"
                "ƒë·ªÉ nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi nh√©"
            )
            return fallback_md, 'unknown', 0.0

        # 3) Semantic expertise cho c√°c category li√™n quan th∆∞ vi·ªán
        allowed = {'huongdan', 'quydinh', 'muon_tra', 'dichvu', 'lienhe'}
        sem_thr = Config.CHAT_SETTINGS.get('semantic_confidence_score', 0.9)
        if category in allowed and conf >= sem_thr:
            return answer or "", category, conf

        # 4) Chat cho c√¢u h·ªèi th√¥ng th∆∞·ªùng (chitchat)
        if category == 'chitchat':
            return answer or "", category, conf

        # 5) Fallback cu·ªëi
        fallback_universal = (
            "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c "
            "ƒë·∫∑t c√¢u h·ªèi t·∫°i [Fanpage th∆∞ vi·ªán](https://www.facebook.com/tvdhtdt) "
            "ƒë·ªÉ ƒë∆∞·ª£c gi·∫£i ƒë√°p nh√©"
        )
        return fallback_universal, 'unknown', 0.0

    def validate_match(self, user_question: str, matched_question: str, score: float) -> bool:
        # Add specific validation for plagiarism and publication questions
        if any(term in user_question.lower() for term in ['tr√πng l·∫∑p', 'ƒë·∫°o vƒÉn', 'sao ch√©p', 'plagiarism']):
            # Require both plagiarism and publication terms for better accuracy
            has_plagiarism = any(term in user_question.lower() for term in ['tr√πng l·∫∑p', 'ƒë·∫°o vƒÉn', 'sao ch√©p', 'plagiarism'])
            has_publication = any(term in user_question.lower() for term in ['c√¥ng b·ªë', 'b√†i b√°o', 'publication'])
            
            if not (has_plagiarism and has_publication):
                return False
            
        return True

# Instantiate matcher and chatbot
faq_matcher = FAQMatcher()
chatbot = Reflection(db_path=Config.DB_PATH)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
@rate_limit
def chat():
    try:
        data = request.get_json() or {}
        msg = data.get("message", "").strip()
        # Ignore session_id since we don't want to maintain chat history
        if not msg:
            return jsonify({"error": Config.MESSAGES['empty_message']}), 400

        answer, category, confidence = faq_matcher.get_best_response(msg)
        return jsonify(ChatResponse(
            message=answer,
            category=category,
            confidence=confidence
        ).to_dict())
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}", exc_info=True)
        return jsonify({"error": Config.MESSAGES['server_error']}), 500

@app.route('/favicon.ico')
def favicon():
    return send_from_directory(
        os.path.dirname(os.path.abspath(__file__)),
        'favicon.ico',
        mimetype='image/vnd.microsoft.icon'
    )

@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Not found"}), 404

@app.errorhandler(500)
def server_error(e):
    logger.error(f"Server error: {e}", exc_info=True)
    return jsonify({"error": Config.MESSAGES['server_error']}), 500

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=Config.DEBUG_MODE)
