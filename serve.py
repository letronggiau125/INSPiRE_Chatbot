import os
import json
import re
import unicodedata
from typing import Optional, Tuple, List, Dict, Any
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv
from difflib import SequenceMatcher
from rapidfuzz import fuzz, process

from rag.ingest_faq import ingest_all_faqs
from reflection import Reflection
from semantic_router.semantic import SemanticSearch
from embedding_model.embedding import EmbeddingModel
from config import Config
from utils.logger import setup_logger
from utils.rate_limiter import rate_limit
from utils.validators import MessageValidator
from utils.response import ChatResponse

# Load environment variables
load_dotenv()
default_db = './chroma_db'

# Ingest all FAQ data into ChromaDB before starting the service
print("üîÑ Ingesting all FAQ data into ChromaDB...")
ingest_all_faqs()
print("‚úÖ FAQ ingest completed.")

# Initialize paths and services
db_path = os.getenv('DB_PATH', default_db)

# Initialize Flask app
app = Flask(
    __name__,
    static_url_path='/static',
    static_folder='static',
    template_folder='templates'
)
CORS(app, resources={
    r"/*": {
        "origins": ["http://127.0.0.1:5000", "http://localhost:5000"],
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type", "Accept"]
    }
})

# Setup logging
logger = setup_logger()

# Initialize embedding and semantic search models
embedding_model = EmbeddingModel()
semantic_search = SemanticSearch()

class FAQMatcher:
    def __init__(self):
        # Blacklist phrases that should be filtered out immediately
        self.no_answer_phrases = [
            'th·ªùi ti·∫øt','ch√≠nh tr·ªã','th·ªÉ thao','kinh t·∫ø vƒ© m√¥','ƒë·∫£ng ph√°i','livestream','3D','government policy','ch√≠nh s√°ch nh√† n∆∞·ªõc','tourism','du l·ªãch','hotel','kh√°ch s·∫°n','c√° ƒë·ªô','ƒë·∫∑t c∆∞·ª£c'
            'vi·ªát nam c√≥ bao nhi√™u t·ªânh','to√°n h·ªçc','v·∫≠t l√Ω','c·ªù b·∫°c','ƒëua xe','h√†ng gi·∫£','LGBT','how many provinces does vietnam have','vi·ªát nam c√≥ bao nhi√™u t·ªânh','weather','th·ªùi ti·∫øt','climate change','bi·∫øn ƒë·ªïi kh√≠ h·∫≠u',
            'h√≥a h·ªçc','sinh h·ªçc','l·ªãch s·ª≠','ƒë·ªãa l√Ω','vƒÉn h·ªçc','kim ti√™m','h·ªçc ph√≠','gi√° cao','population of vietnam','d√¢n s·ªë vi·ªát nam','flight','chuy·∫øn bay','c·∫£ng h√†ng kh√¥ng','c√° c∆∞·ª£c','t·ª≠ s√°t'
            't√™n b·∫°n l√† g√¨','b·∫°n bao nhi√™u tu·ªïi', 'b·∫°n s·ªëng ·ªü ƒë√¢u','l√†m ti·ªÅn','b√™ ƒë√™','qu·ªëc ph√≤ng','stock market', 'ch·ªâ s·ªë ch·ª©ng kho√°n','bus schedule', 'l·ªãch xe bu√Ωt', 'giao th√¥ng','t·ª± t·ª≠'
            'b·∫°n c√≥ ng∆∞·ªùi y√™u ch∆∞a','b·∫°n ƒë√£ k·∫øt h√¥n ch∆∞a','c√°ch m·∫°ng','covid','qu√¢n s·ª±','bi·ªÉn','ƒë·∫£o','inflation rate', 't·ª∑ l·ªá l·∫°m ph√°t','music festival', 'l·ªÖ h·ªôi √¢m nh·∫°c','sports','th·ªÉ thao','football', 'b√≥ng ƒë√°', 'basketball',
            'b·∫°n ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o', 'thu·∫≠t to√°n c·ªßa b·∫°n l√† g√¨','ƒë·∫£o ch√≠nh','c·∫ßm t√π','ho√†ng xa','tr∆∞·ªùng xa','exchange rate', 't·ª∑ gi√° ngo·∫°i t·ªá','c√¥ng th·ª©c n·∫•u ƒÉn','b√™ ƒë√™','thi√™u s·ªëng'
            'b·∫°n h·ªçc nh∆∞ th·∫ø n√†o','b·∫°n l√† model g√¨', 'd·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa b·∫°n l√† g√¨','ƒë√°nh','khai chi·∫øn','concert', 'bu·ªïi h√≤a nh·∫°c', 'movie', 'phim','health', 's·ª©c kh·ªèe', 'y t·∫ø',
            'nh√† h√†ng','cƒÉng tin','k√Ω t√∫c x√°', 'nh√† ·ªü','c·ªìn','ch·∫•t k√≠ch th√≠ch','m·∫°i d√¢m','xung ƒë·ªôt','qu·ªëc gia', 'd√¢n t·ªôc','how do you work', 'bot ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o','gi·∫øt'
            'giao th√¥ng','l·ªãch xe bu√Ωt','b√£i ƒë·∫≠u xe','l·ªãch thi','bia','politics', 'ch√≠nh tr·ªã', 'ƒë·∫£ng ph√°i', 'kinh t·∫ø vƒ© m√¥','exercise', 'th·ªÉ d·ª•c', 'gym','nh·∫°y c·∫£m','ch√©m','ƒë√¢m'
        ]

        # Common typos mapping
        self.common_typos = {
            'th·ªùi ti·∫øt': ['thoi tiet', 'th·ªùi ti·∫øt', 'th·ªùi ti√™t'],
            'ch√≠nh tr·ªã': ['chinh tri', 'ch√≠nh tr·ªã', 'ch√≠nh tr·ªã'],
            'th·ªÉ thao': ['the thao', 'th·ªÉ thao', 'th·ªÉ thao'],
            'du l·ªãch': ['du lich', 'du l·ªãch', 'du l·ªãch'],
            'kh√°ch s·∫°n': ['khach san', 'kh√°ch s·∫°n', 'kh√°ch s·∫°n'],
            'th·ªÉ d·ª•c': ['the duc', 'th·ªÉ d·ª•c', 'th·ªÉ d·ª•c'],
            'giao th√¥ng': ['giao thong', 'giao th√¥ng', 'giao th√¥ng'],
            'ch√≠nh s√°ch': ['chinh sach', 'ch√≠nh s√°ch', 'ch√≠nh s√°ch'],
            'qu·ªëc gia': ['quoc gia', 'qu·ªëc gia', 'qu·ªëc gia'],
            'd√¢n t·ªôc': ['dan toc', 'd√¢n t·ªôc', 'd√¢n t·ªôc'],
            'Th∆∞ vi·ªán': ['thu vien', 'th∆∞ vi·ªán','th∆∞ vi·ªán inspire',"INSPiRE",'th∆∞ vi·ªán INSPiRE','Th∆∞ vi·ªán Truy·ªÅn c·∫£m h·ª©ng'],
            'Tr∆∞·ªùng ƒê·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': ['tdtu', 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng', 'TDTU','tr∆∞·ªùng tdt','Tr∆∞·ªùng Ton Duc Thang','ƒë·∫°i h·ªçc Ton Duc Thang'],
            'ƒê·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': ['tdtu', 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng', 'TDTU'],
        }   

        # Ignored patterns for super normalization
        self.IGNORED_PATTERNS = [
            r'\b·ªü th∆∞ l√Ω\b', r'\b√†i th∆∞ l√Ω\b', r'\btrong th∆∞ l√Ω\b',
            r'\btoi muon\b', r'\btoi can\b', r'\bgiu\b', r'\bcho em h·ªèi\b', r'\bxin h·ªèi\b', r'\bl√†m xin\b', r'\bxin\b',
            r'\bbai l√†\b', r'\bai ƒëang\b', r'\bai r·ªìi\b', r'\bai ƒëi\b', r'\bai ƒëi ch·ªß\b',
            r'\bnay v·ªÅ\b', r'\bchi ‚Äã‚Äã‚Äã‚Äãti·∫øt\b', r'\b th√¥ng tin v·ªÅ\b',
            r'\bvui l√≤ng\b', r'\bl√†m ∆°n\b', r'\bgi√∫p\b', r'\bgi√∫p t√¥i\b',
            r'\bcho t√¥i bi·∫øt\b', r'\bcho t√¥i h·ªèi\b', r'\bcho h·ªèi\b',
            r'\bc√≥ th·ªÉ\b', r'\bc√≥ ƒë∆∞·ª£c kh√¥ng\b', r'\bc√≥ ph·∫£i\b',
            r'\bƒë∆∞·ª£c kh√¥ng\b', r'\bph·∫£i kh√¥ng\b', r'\bƒë√∫ng kh√¥ng\b'
        ]

        # Synonym mapping for better matching
        self.SYNONYM_MAP = {
            'm∆∞·ª£n': ['vay', 'l·∫•y', 'nh·∫≠n', 'ƒë·ªçc'],
            'tr·∫£': ['ho√†n tr·∫£', 'n·ªôp l·∫°i', 'g·ª≠i l·∫°i'],
            't√†i li·ªáu': ['s√°ch', 's√°ch b√°o', 'gi√°o tr√¨nh', 't√†i li·ªáu h·ªçc t·∫≠p'],
            'gi·∫£ng vi√™n': ['th·∫ßy c√¥', 'gi√°o vi√™n', 'gi·∫£ng vi√™n'],
            'sinh vi√™n': ['h·ªçc vi√™n', 'sinh vi√™n'],
            'th∆∞ vi·ªán': ['th∆∞ vi·ªán tr∆∞·ªùng', 'th∆∞ vi·ªán tdtu'],
            'm√°y t√≠nh': ['m√°y vi t√≠nh', 'computer', 'pc'],
            'm√°y in': ['printer', 'm√°y photocopy'],
            'wifi': ['internet', 'm·∫°ng', 'k·∫øt n·ªëi m·∫°ng'],
            'ƒëƒÉng nh·∫≠p': ['login', 'ƒëƒÉng k√Ω', 'truy c·∫≠p'],
            't√†i kho·∫£n': ['account', 'user'],
            'm·∫≠t kh·∫©u': ['password', 'pass'],
            'ph√≠': ['gi√°', 'chi ph√≠', 'ti·ªÅn'],
            'gia h·∫°n': ['renew', 'k√©o d√†i', 'th√™m th·ªùi gian'],
            'quy ƒë·ªãnh': ['n·ªôi quy', 'lu·∫≠t', 'quy t·∫Øc'],
            'd·ªãch v·ª•': ['service', 'ti·ªán √≠ch'],
            'h∆∞·ªõng d·∫´n': ['guide', 'manual', 'tutorial'],
            'li√™n h·ªá': ['contact', 'g·∫∑p', 'g·∫∑p m·∫∑t'],
            'm·∫•t': ['th·∫•t l·∫°c', 'qu√™n', 'kh√¥ng t√¨m th·∫•y'],
            't√¨m': ['search', 't√¨m ki·∫øm', 'locate']
        }

        # Library domain keywords
        self.LIBRARY_KEYWORDS = {
            'library', 'book', 'borrow', 'return', 'renew','ph√≤ng ch·ª©c nƒÉng',
            'resources', 'wifi', 'computer', 'printer','inspire','INSPiRE',
            'find', 'lost item', 'library card','Th∆∞ vi·ªán Truy·ªÅn c·∫£m h·ª©ng',
            'contact', 'rules', 'services', 'guidelines',
            'th∆∞ vi·ªán', 's√°ch', 'm∆∞·ª£n', 'tr·∫£', 'ƒë·ªçc', 't√†i li·ªáu',
            'sinh vi√™n', 'gi·∫£ng vi√™n', 'tr∆∞·ªùng', 'ƒë·∫°i h·ªçc',
            'tdtu', 'th·∫ª', 'ph√≠', 'ph·∫°t', 'gia h·∫°n', 'ƒëƒÉng k√Ω',
            't√†i kho·∫£n', 'c∆° s·ªü', 'ph√≤ng', 'gi·ªù', 'm·ªü c·ª≠a',
            'ƒë√≥ng c·ª≠a', 'd·ªãch v·ª•', 'nghi√™n c·ª©u', 'h·ªçc t·∫≠p',
            'thi', 'k·ª≥ thi', 'khoa', 'ng√†nh','TVƒêHTƒêT'
        }

        # Expanded tech terms mapping
        self.tech_terms = {
            'm√°y t√≠nh': ['computer', 'laptop', 'pc'],
            'm√°y in': ['printer'],
            'm√°y qu√©t': ['scanner'],
            'photo': ['copy'],
            'wifi': ['internet'],
            't·∫£i': ['download', 'upload'],
            'ƒëƒÉng nh·∫≠p': ['login'],
            't√†i kho·∫£n': ['account'],
            'm·∫≠t kh·∫©u': ['password'],
            # Reference management software
            'endnote': 'huongdan',
            'zotero': 'huongdan',
            'mendeley': 'huongdan',
            'reference manager': 'huongdan',
            'citation': 'huongdan',
            'tr√≠ch d·∫´n': 'huongdan',
            't√†i li·ªáu tham kh·∫£o': 'huongdan',
            # Database access
            'database': 'huongdan',
            'c∆° s·ªü d·ªØ li·ªáu': 'huongdan',
            'proquest': 'huongdan',
            'ebsco': 'huongdan',
            'ieee': 'huongdan',
            'science direct': 'huongdan',
            'springer': 'huongdan',
            # Research tools
            'turnitin': 'huongdan',
            'plagiarism': 'huongdan',
            'ƒë·∫°o vƒÉn': 'huongdan',
            'research': 'huongdan',
            'nghi√™n c·ª©u': 'huongdan'
        }
        self.abbrev_map = {
            'tdt': 'tr∆∞·ªùng ƒë·∫°i h·ªçc t√¥n ƒë·ª©c th·∫Øng',
            'tv': 'th∆∞ vi·ªán',
            'sv': 'sinh vi√™n',
            'gv': 'Gi·∫£ng vi√™n',
            'GV-VC': 'Gi·∫£ng vi√™n-Vi√™n ch·ª©c',
            'c·ª±u sv': 'C·ª±u Sinh vi√™n',
            'hvch': 'H·ªçc vi√™n cao h·ªçc',
            'ncs' : 'Ngi√™n c·ª©u sinh',
            'ht' : 'Hi·ªáu tr∆∞·ªüng',
            'csdl': 'C∆° s·ªü d·ªØ li·ªáu',
            'CSDL' : 'C∆° s·ªü d·ªØ li·ªáu',
            'th·∫ª sv': 'Th·∫ª sinh vi√™n',
            'th·∫ª tv': 'Th·∫ª th∆∞ vi·ªán',
            'database': 'C∆° s·ªü d·ªØ li·ªáu',
            'copyright': 'B·∫£n quy·ªÅn',
            'Th∆∞ vi·ªán ƒë·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng': 'TVƒêHTƒêT'
        }
        self.keyword_map = {
            'printer': 'dichvu',
            'scanner': 'huongdan',
            'wifi': 'dichvu',
            'download': 'huongdan',
            'upload': 'huongdan',
            'login': 'huongdan',
            'lost': 'lienhe',
            'm·∫•t': 'lienhe',
            'th·∫•t l·∫°c': 'lienhe',
            'qu√™n': 'lienhe',
            'forgot': 'lienhe',
            'missing': 'lienhe'
        }

        # Regex patterns to "predict" categories from user queries
        self.question_patterns = {
            'instructions': [r'(how|how do i|how can i)', r'cach .*', r'where .*', r'where .*'],
            'quydinh': [r'quy ƒë·ªãnh .*', r'(co c√≥|d·ª©c ƒë∆∞·ª£c|d·ª©c ƒë∆∞·ª£c|rules).*'],
            'dichvu': [r'ƒëi·ªÅu sƒ© .*', r'(ph√¨|gi√°|services).*'],
            'muon_tra': [r'(m∆∞·ª£n|tu·∫ßn| xu√¢n|borrow|return|renew).*'],
            'lianhe': [r'(li·ªÖn li√™n|contact|g·∫£i ai).*'],
            'chitchat': [r'(hello|hi|hello|goodbye|bye).*']
        }
        
        # Strict chitchat patterns - only basic greetings and thanks
        self.chitchat_patterns = {
            r'\b(xin ch√†o|hello|hi|hey|ch√†o)\b': 'greeting',
            r'\b(c·∫£m ∆°n|thank|thanks|c√°m ∆°n)\b': 'thanks',
            r'\b(t·∫°m bi·ªát|bye|goodbye)\b': 'goodbye'
        }

        # Load FAQ data and build hash map for direct lookup
        self.faq_data = self.load_faq_data()
        
        # Build normalized questions and aliases for fuzzy matching
        self.norm_questions = [
            (self.super_normalize(self.expand_synonyms(q['question'])), i)
            for i, q in enumerate(self.faq_data)
        ]
        self.norm_aliases = [
            (self.super_normalize(self.expand_synonyms(alias)), i)
            for i, q in enumerate(self.faq_data)
            for alias in q.get('aliases', [])
        ]

        # Build semantic index on FAQ questions
        questions = [entry['question'] for entry in self.faq_data]
        semantic_search.build_index(questions)

        # Define allowed specialized categories for semantic search
        self.allowed_specialized = {'huongdan', 'quydinh', 'muon_tra', 'dichvu', 'lienhe'}

    def remove_accents(self, text: str) -> str:
        """Remove Vietnamese accents from text."""
        nfkd_form = unicodedata.normalize('NFKD', text)
        return "".join([c for c in nfkd_form if not unicodedata.combining(c)])

    def is_blocked_phrase(self, user_message: str, threshold: int = 80) -> bool:
        """Check if user message contains any blocked phrases using fuzzy matching."""
        if not user_message:
            return False
            
        user_message_lower = user_message.lower().strip()
        user_message_no_accents = self.remove_accents(user_message_lower)
        
        # List of library-related terms that should never be blocked
        library_terms = {
            's√°ch', 'sach', 'book', 'books',
            'ph√≤ng', 'phong', 'room', 'rooms',
            'th∆∞ vi·ªán', 'thu vien', 'library',
            'm∆∞·ª£n', 'muon', 'borrow',
            'tr·∫£', 'tra', 'return',
            'ƒë·ªçc', 'doc', 'read',
            't√†i li·ªáu', 'tai lieu', 'document',
            'gi√°o tr√¨nh', 'giao trinh', 'textbook',
            't·∫°p ch√≠', 'tap chi', 'journal',
            'b√°o', 'bao', 'newspaper',
            'm√°y t√≠nh', 'may tinh', 'computer',
            'm√°y in', 'may in', 'printer',
            'wifi', 'internet', 'm·∫°ng',
            'ƒëƒÉng k√Ω', 'dang ky', 'register',
            'ƒëƒÉng nh·∫≠p', 'dang nhap', 'login',
            't√†i kho·∫£n', 'tai khoan', 'account',
            'm·∫≠t kh·∫©u', 'mat khau', 'password',
            'th·∫ª', 'the', 'card',
            'ph√≠', 'phi', 'fee',
            'gia h·∫°n', 'gia han', 'renew',
            'quy ƒë·ªãnh', 'quy dinh', 'rule',
            'd·ªãch v·ª•', 'dich vu', 'service',
            'h∆∞·ªõng d·∫´n', 'huong dan', 'guide',
            'li√™n h·ªá', 'lien he', 'contact'
        }

        # List of sensitive terms that should always be blocked
        sensitive_terms = {
            'nh·∫°y c·∫£m', 'nhay cam', 'sensitive',
            'ch√≠nh tr·ªã', 'chinh tri', 'politics',
            'ƒë·∫£ng ph√°i', 'dang phai', 'political party',
            'kinh t·∫ø vƒ© m√¥', 'kinh te vi mo', 'macroeconomic',
            'th·ªÉ d·ª•c', 'the duc', 'exercise',
            'gym', 'fitness', 
            'giao th√¥ng', 'giao thong', 'traffic',
            'l·ªãch xe bu√Ωt', 'lich xe buyt', 'bus schedule',
            'b√£i ƒë·∫≠u xe', 'bai dau xe', 'parking',
            'l·ªãch thi', 'lich thi', 'exam schedule',
            'bia', 'beer','c·∫ßm ƒë·ªì'
            'nh√† h√†ng', 'nha hang', 'restaurant',
            'cƒÉng tin', 'cang tin', 'canteen',
            'k√Ω t√∫c x√°', 'ky tuc xa', 'dormitory',
            'nh√† ·ªü', 'nha o', 'housing',
            'c·ªìn', 'con', 'alcohol',
            'ch·∫•t k√≠ch th√≠ch', 'chat kich thich', 'stimulant',
            'm·∫°i d√¢m', 'mai dam', 'prostitution',
            'xung ƒë·ªôt', 'xung dot', 'conflict',
            'qu·ªëc gia', 'quoc gia', 'nation',
            'd√¢n t·ªôc', 'dan toc', 'ethnicity'
        }
        
        # First check for sensitive terms - these should always be blocked
        for term in sensitive_terms:
            if term in user_message_lower or term in user_message_no_accents:
                logger.info(f"Blocked sensitive term: '{term}'")
                return True
                
        # Then check exact matches for other blocked phrases
        for phrase in self.no_answer_phrases:
            if phrase in user_message_lower:
                # Skip if the phrase is part of a library term and not a sensitive term
                if any(term in user_message_lower for term in library_terms) and not any(term in user_message_lower for term in sensitive_terms):
                    continue
                return True
                
        # Then check fuzzy matches with higher threshold for longer phrases
        for phrase in self.no_answer_phrases:
            # Skip short phrases to avoid false positives
            if len(phrase) < 4:
                continue
                
            # Skip if the phrase is part of a library term and not a sensitive term
            if any(term in phrase for term in library_terms) and not any(term in phrase for term in sensitive_terms):
                continue
                
            # Calculate threshold based on phrase length
            phrase_threshold = min(threshold, 85) if len(phrase) > 10 else threshold
            
            # Check original phrase
            ratio = fuzz.partial_ratio(phrase, user_message_lower)
            if ratio >= phrase_threshold:
                # Double check that it's not a library term or is a sensitive term
                if not any(term in user_message_lower for term in library_terms) or any(term in user_message_lower for term in sensitive_terms):
                    logger.info(f"Blocked phrase match: '{phrase}' with ratio {ratio}")
                    return True

            # Check phrase without accents
            phrase_no_accents = self.remove_accents(phrase)
            ratio = fuzz.partial_ratio(phrase_no_accents, user_message_no_accents)
            if ratio >= phrase_threshold:
                # Double check that it's not a library term or is a sensitive term
                if not any(term in user_message_no_accents for term in library_terms) or any(term in user_message_no_accents for term in sensitive_terms):
                    logger.info(f"Blocked phrase match (no accents): '{phrase}' with ratio {ratio}")
                    return True

            # Check common typos
            if phrase in self.common_typos:
                for typo in self.common_typos[phrase]:
                    ratio = fuzz.partial_ratio(typo, user_message_lower)
                    if ratio >= phrase_threshold:
                        # Double check that it's not a library term or is a sensitive term
                        if not any(term in user_message_lower for term in library_terms) or any(term in user_message_lower for term in sensitive_terms):
                            logger.info(f"Blocked phrase match (typo): '{phrase}' with ratio {ratio}")
                            return True

            # Only check individual words for longer phrases
            if len(phrase.split()) > 1:
                words = phrase.split()
                for word in words:
                    if len(word) < 4:  # Skip short words
                        continue
                    # Skip if the word is a library term and not a sensitive term
                    if word in library_terms and word not in sensitive_terms:
                        continue
                    ratio = fuzz.partial_ratio(word, user_message_lower)
                    if ratio >= 90:  # Higher threshold for word matches
                        # Double check that it's not part of a library term or is a sensitive term
                        if not any(term in user_message_lower for term in library_terms) or any(term in user_message_lower for term in sensitive_terms):
                            logger.info(f"Blocked word match: '{word}' with ratio {ratio}")
                            return True

        return False

    def super_normalize(self, text: str) -> str:
        """Super normalize text by removing auxiliary phrases and normalizing."""
        t = text.lower().strip()
        t = re.sub(r'[^\w\s]', ' ', t)
        for p in self.IGNORED_PATTERNS:
            t = re.sub(p, ' ', t)
        t = re.sub(r'\s+', ' ', t)
        return t.strip()

    def expand_synonyms(self, text: str) -> str:
        """Expand synonyms in text to their canonical form."""
        t = text.lower()
        for canonical, synonyms in self.SYNONYM_MAP.items():
            for syn in synonyms:
                t = t.replace(syn, canonical)
        return t

    def fuzzy_lookup(self, norm_user: str, norm_questions: List[Tuple[str, int]], threshold: int = 85) -> Optional[int]:
        """Find best fuzzy match using rapidfuzz."""
        best_ratio = 0
        best_idx = None
        
        for norm, idx in norm_questions:
            ratio = fuzz.ratio(norm_user, norm)
            if ratio > best_ratio and ratio >= threshold:
                best_ratio = ratio
                best_idx = idx
                
        return best_idx

    def in_library_domain(self, text: str) -> bool:
        """Check if the text is related to library domain."""
        text = text.lower()
        return any(kw in text for kw in self.LIBRARY_KEYWORDS)

    def load_faq_data(self) -> List[Dict[str, Any]]:
        try:
            with open("faq_data.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                return data
        except Exception as e:
            logger.error(f"Error loading FAQ data: {e}")
            return []

    def normalize_text(self, text: str) -> str:
        """Normalize text by removing accents, punctuation, and extra whitespace."""
        # 1) NFKD for accent separation
        text = unicodedata.normalize('NFKD', text)
        text = ''.join(ch for ch in text if not unicodedata.combining(ch))
        # 2) Lower, strip punctuation
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # remove .,;?‚Ä¶
        text = re.sub(r'\s+', ' ', text).strip()  # collect whitespace
        return text

    def build_faq_map(self) -> Dict[str, Dict[str, Any]]:
        faq_map = {}
        for entry in self.faq_data:
            # Normalize main question
            raw = entry['question']
            norm = self.normalize_text(raw)
            faq_map[norm] = entry
            
            # Normalize aliases too
            for alias in entry.get('aliases', []):
                norm_alias = self.normalize_text(alias)
                faq_map[norm_alias] = entry
        return faq_map

    def normalize_abbreviations(self, text: str) -> str:
        q = text.lower().strip()
        # Remove trailing punctuation
        q = re.sub(r'[?!.]+$', '', q)
        # Expand abbreviations
        for abbr, full in self.abbrev_map.items():
            q = re.sub(rf"\b{re.escape(abbr)}\b", full, q)
        return q

    def find_best_match(self, user_question: str, threshold: float = 0.5) -> Dict[str, Any]:
        """Find the best matching FAQ entry for a user question."""
        # 1. Check for blocked phrases using fuzzy matching
        if self.is_blocked_phrase(user_question):
            logger.info(f"Question blocked: {user_question}")
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0, '_no_answer': True}

        # 2. Super normalize and expand synonyms
        user_norm = self.super_normalize(self.expand_synonyms(user_question))
        logger.info(f"Normalized question: {user_norm}")

        # 3. Direct matches (exact match after normalization)
        for norm, idx in self.norm_questions + self.norm_aliases:
            if user_norm == norm:
                entry = self.faq_data[idx].copy()
                entry['confidence'] = 1.0
                logger.info(f"Found exact match: {entry['question']}")
                return entry

        # 4. Fuzzy matches
        idx = self.fuzzy_lookup(user_norm, self.norm_questions + self.norm_aliases)
        if idx is not None:
            entry = self.faq_data[idx].copy()
            entry['confidence'] = 0.95
            logger.info(f"Found fuzzy match: {entry['question']}")
            return entry

        # 5. Domain filter: if not in library domain, ignore semantics
        if not self.in_library_domain(user_norm):
            logger.info(f"Question not in library domain: {user_norm}")
            return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

        # 6. Semantic match only for allowed categories
        try:
            results = semantic_search.query(user_question, top_k=1, threshold=threshold)
            logger.info(f"Semantic search results for '{user_question}': {results}")
            
            if results and len(results) > 0:
                idx, score = results[0]['index'], float(results[0]['score'])
                matched = self.faq_data[idx].copy()
                logger.info(f"Top semantic match: idx={idx}, score={score}, category={matched.get('category')}")
                
                # Only accept if category is allowed and score is high enough
                if matched.get('category') in self.allowed_specialized and score >= threshold:
                    matched['confidence'] = score
                    return matched
                else:
                    logger.info(f"Semantic match rejected: category={matched.get('category')}, score={score}")
        except Exception as e:
            logger.error(f"Error in semantic matching: {e}", exc_info=True)

        # 7. No match found
        logger.info(f"No match found for: {user_question}")
        return {'question': user_question, 'category': 'unknown', 'confidence': 0.0}

    def get_best_response(self, user_message: str) -> Tuple[str, str, float]:
        match = self.find_best_match(user_message)
        
        # If caught no-answer at yes/no step
        if match.get('_no_answer'):
            return "Xin l·ªói, t√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi n√†y.", 'unknown', 0.0

        answer = match.get('answer')
        category = match.get('category')
        conf = match.get('confidence', 0.0)

        # 1) FAQ exact/alias
        if conf == 1.0 and answer:
            return answer, category, conf

        # 2) FAQ category keyword but no answer
        if conf == 1.0 and not answer:
            return "Xin l·ªói, t√¥i ch∆∞a c√≥ c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi n√†y.", 'unknown', 0.0

        # 3) Semantic expertise
        allowed = {'huongdan', 'quydinh', 'muon_tra', 'dichvu', 'lienhe'}
        sem_thr = Config.CHAT_SETTINGS.get('semantic_confidence_score', 0.9)
        if category in allowed and conf >= sem_thr:
            return answer or "", category, conf

        # 4) Chat
        if category == 'chitchat':
            ans = Reflection(db_path=db_path).chat(None, user_message)
            return ans, 'chatbot', 0.0

        # 5) Fallback cu·ªëi
        return "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c ƒë·∫∑t c√¢u h·ªèi t·∫°i fanpage th∆∞ vi·ªán (https://www.facebook.com/tvdhtdt) ƒë·ªÉ ƒë∆∞·ª£c gi·∫£i ƒë√°p nh√©", 'unknown', 0.0

# Instantiate matcher and chatbot
faq_matcher = FAQMatcher()
chatbot = Reflection(db_path=Config.DB_PATH)

@app.route("/")
def home():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
@rate_limit
def chat():
    try:
        data = request.get_json() or {}
        msg = data.get("message", "").strip()
        session_id = data.get("session_id", "default_session")
        if not msg:
            return jsonify({"error": Config.MESSAGES['empty_message']}), 400

        answer, category, confidence = faq_matcher.get_best_response(msg)
        return jsonify(ChatResponse(
            message=answer,
            category=category,
            confidence=confidence
        ).to_dict())
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}", exc_info=True)
        return jsonify({"error": Config.MESSAGES['server_error']}), 500

@app.route('/favicon.ico')
def favicon():
    return send_from_directory(
        os.path.dirname(os.path.abspath(__file__)),
        'favicon.ico',
        mimetype='image/vnd.microsoft.icon'
    )

@app.errorhandler(404)
def not_found(e):
    return jsonify({"error": "Not found"}), 404

@app.errorhandler(500)
def server_error(e):
    logger.error(f"Server error: {e}", exc_info=True)
    return jsonify({"error": Config.MESSAGES['server_error']}), 500

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=Config.DEBUG_MODE)
